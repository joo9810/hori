{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5c579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2524bec019af4cbaad03430f5efb7c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"../model/kanana-1.5-8b-instruct-2505\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\", trust_remote_code=True)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"dtype\": torch.bfloat16},\n",
    "    device=\"cuda:0\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "941601d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you give me some tips on how to prevent gum disease?\n"
     ]
    }
   ],
   "source": [
    "pre_query_template = \"[|system|]You are a helpful assistant.[|endofturn|]\\n[|Korean user|]\"\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"[|endofturn|]\"),\n",
    "]\n",
    "\n",
    "instruction = pipeline(\n",
    "    pre_query_template,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    ")\n",
    "\n",
    "sanitized_instruction = instruction[0]['generated_text'][len(pre_query_template):].split(\"\\n\")[0]\n",
    "print(sanitized_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac743379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer, BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import threading\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92340686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a2949f819045938fb04afc61123124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128259, 4096, padding_idx=128001)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dir = \"../model/kanana-1.5-8b-instruct-2505\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_dir,\n",
    "    padding_side=\"right\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ef5029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n안녕?<|eot_id|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template([{'role':'user','content':'안녕?'}], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b712085e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Problem:\n",
      "Implement a function that takes in a list of integers and returns a dictionary where the keys are the input integers and the values are their corresponding squares.\n",
      "\n",
      "# Example:\n",
      "For the input `[1, 2, 3]`, the output should be `{1: 1, 2: 4, 3: 9}`.\n",
      "\n",
      "# Constraints:\n",
      "# - Input can contain duplicates.\n",
      "# - The function should handle both positive and negative integers.\n",
      "\n",
      "# Solution in Python:\n",
      "```python\n",
      "def square_dictionary(numbers):\n",
      "    return {num: num ** 2 for num in numbers}\n",
      "```\n",
      "\n",
      "# Explanation:\n",
      "This solution uses a dictionary comprehension to efficiently create the desired dictionary. For each number in the input list, it adds an entry to the dictionary with the number as the key and its square as the value. Duplicates are handled naturally, as each number is only added once with its square value.\n",
      "\n",
      "The solution is efficient in both time and space complexity, as it iterates through the list once and uses a constant amount of additional space per element."
     ]
    }
   ],
   "source": [
    "pre_query_template = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\\n<|eot_id|><|start_header_id|>Korean user<|end_header_id|>\\n\\n\"\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "streamer = TextIteratorStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    pre_query_template,\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids.to(model.device)\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "thread = threading.Thread(\n",
    "    target=model.generate,\n",
    "    kwargs=generation_kwargs,\n",
    ")\n",
    "thread.start()\n",
    "\n",
    "generated_text = \"\"\n",
    "\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    generated_text += new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b418dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[FINAL]\n",
      "안녕하세요! 인공지능에 대해 관심이 많습니다. 특히 인공지능의 발전 과정을 이해하고 싶은데, 그 발전 과정을 간략히 요약해주실 수 있을까요?\n"
     ]
    }
   ],
   "source": [
    "sanitized_instruction = generated_text.split(\"\\n\")[0]\n",
    "print(\"\\n\\n[FINAL]\")\n",
    "print(sanitized_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3804c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
