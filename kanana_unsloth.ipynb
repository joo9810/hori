{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be328cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch, psutil, os, gc\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from MyModule import get_current_time, get_current_date, get_weather, search_address, get_alarms, set_alarms, delete_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8df172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:22<00:00, 50.62s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"../model/kanana-1.5-8b-instruct-2505\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93aeadbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2026.1.3 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb461cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user', 'content': 'ìˆ˜ë„ìš”ê¸ˆì€ ì–´ë””ì— ë¬¸ì˜í•´?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'ìˆ˜ë„ìš”ê¸ˆì€ ì§€ì—­ ìˆ˜ë„ì‚¬ì—…ì†Œë‚˜ ì‹œì²­ ê´€ë ¨ ë¶€ì„œì— ë¬¸ì˜í•˜ë©´ ë¼ìš”. ê³ ì§€ì„œì— ì íŒ ì „í™”ë²ˆí˜¸ë¡œ ì—°ë½í•˜ë©´ ê°€ì¥ ë¹ ë¥´ê²Œ ì•ˆë‚´ë°›ì„ ìˆ˜ ìˆì–´ìš”.'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = load_dataset(\"json\", data_files=\"./data/train_data_singleturn.jsonl\")\n",
    "dataset2 = load_dataset(\"json\", data_files=\"./data/train_data_function_call_kanana.jsonl\")\n",
    "# dataset3 = load_dataset(\"json\", data_files=\"./data/train_data_function_call_alarm.jsonl\")\n",
    "\n",
    "dataset = concatenate_datasets([\n",
    "    dataset1['train'],\n",
    "    dataset2['train'],\n",
    "    # dataset3['train'],\n",
    "]).shuffle(seed=42)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3d92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "    \"get_alarms\": get_alarms,\n",
    "    \"set_alarms\": set_alarms,\n",
    "    \"delete_alarm\": delete_alarms,\n",
    "}\n",
    "\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"get_alarms\",\n",
    "    #     \"description\": \"í˜„ì¬ ë“±ë¡ëœ ëª¨ë“  ì•ŒëŒ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤. (ë§¤ê°œë³€ìˆ˜ ì—†ìŒ)\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"set_alarms\",\n",
    "    #     \"description\": \"ìƒˆë¡œìš´ ì•ŒëŒì„ ì˜ˆì•½í•©ë‹ˆë‹¤.\",\n",
    "    #     \"parameters\": {\n",
    "    #         \"type\": \"object\",\n",
    "    #         \"properties\": {\n",
    "    #             \"message\": {\n",
    "    #                 \"type\": \"string\",\n",
    "    #                 \"description\": \"ì•ŒëŒ ì‹œ ì•Œë ¤ì¤„ ë‚´ìš©\"\n",
    "    #             },\n",
    "    #             \"year\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"\n",
    "    #             },\n",
    "    #             \"month\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì›” (1-12)\"\n",
    "    #             },\n",
    "    #             \"day\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì¼ (1-31)\"\n",
    "    #             },\n",
    "    #             \"hour\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì‹œ (0-23)\"\n",
    "    #             },\n",
    "    #             \"minute\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ë¶„ (0-59)\"\n",
    "    #             },\n",
    "    #             \"relative_day\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"\n",
    "    #             },\n",
    "    #             \"week_offset\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"\n",
    "    #             },\n",
    "    #             \"day_of_week\": {\n",
    "    #                 \"type\": \"string\",\n",
    "    #                 \"description\": \"ìš”ì¼ (ì˜ˆ: 'ì›”ìš”ì¼', 'í™”ìš”ì¼', ..., 'ì¼ìš”ì¼')\"\n",
    "    #             },\n",
    "    #             \"is_ampm_specified\": {\n",
    "    #                 \"type\": \"boolean\",\n",
    "    #                 \"description\": \"ì‚¬ìš©ìê°€ 'ì˜¤ì „', 'ì˜¤í›„', 'ì•„ì¹¨', 'ë°¤' ë“± ì‹œê°„ì„ ì§€ì •í–ˆë‹¤ë©´ True, ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´ Falseë¥¼ ì£¼ì„¸ìš”.\"\n",
    "    #             }\n",
    "    #         },\n",
    "    #         \"required\": [\"hour\"]\n",
    "    #     }\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"delete_alarms\",\n",
    "    #     \"description\": \"ë“±ë¡ëœ ì•ŒëŒì„ ì‚­ì œí•©ë‹ˆë‹¤\",\n",
    "    #     \"parameters\": {\n",
    "    #         \"type\": \"object\",\n",
    "    #         \"properties\": {\n",
    "    #             \"message\": {\n",
    "    #                 \"type\": \"string\",\n",
    "    #                 \"description\": \"ì‚­ì œí•  ì•ŒëŒì˜ ë©”ì‹œì§€ ë‚´ìš©\"\n",
    "    #             },\n",
    "    #             \"year\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"\n",
    "    #             },\n",
    "    #             \"month\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì›” (1-12)\"\n",
    "    #             },\n",
    "    #             \"day\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì¼ (1-31)\"\n",
    "    #             },\n",
    "    #             \"hour\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì‹œ (0-23)\"\n",
    "    #             },\n",
    "    #             \"minute\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ë¶„ (0-59)\"\n",
    "    #             },\n",
    "    #             \"relative_day\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"\n",
    "    #             },\n",
    "    #             \"week_offset\": {\n",
    "    #                 \"type\": \"integer\",\n",
    "    #                 \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"\n",
    "    #             },\n",
    "    #             \"day_of_week\": {\n",
    "    #                 \"type\": \"string\",\n",
    "    #                 \"description\": \"ìš”ì¼ (ì˜ˆ: 'ì›”ìš”ì¼', 'í™”ìš”ì¼', ..., 'ì¼ìš”ì¼')\"\n",
    "    #             },\n",
    "    #             \"is_ampm_specified\": {\n",
    "    #                 \"type\": \"boolean\",\n",
    "    #                 \"description\": \"ì‚¬ìš©ìê°€ 'ì˜¤ì „', 'ì˜¤í›„', 'ì•„ì¹¨', 'ë°¤' ë“± ì‹œê°„ì„ ì§€ì •í–ˆë‹¤ë©´ True, ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´ Falseë¥¼ ì£¼ì„¸ìš”.\"\n",
    "    #             }\n",
    "    #         }\n",
    "    #     }\n",
    "    # }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41b4a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5572/5572 [00:00<00:00, 8932.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\\n\\nYou have access to the following functions:\\n\\nUse the function \\'get_current_time\\' to \\'í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\\'\\n{\"name\": \"get_current_time\", \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}\\n\\nUse the function \\'get_current_date\\' to \\'í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\\'\\n{\"name\": \"get_current_date\", \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}\\n\\nUse the function \\'get_weather\\' to \\'íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\\'\\n{\"name\": \"get_weather\", \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"ì§€ì—­ ì´ë¦„\"}}, \"required\": [\"city\"]}}\\n\\nUse the function \\'search_address\\' to \\'ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\\'\\n{\"name\": \"search_address\", \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"ê²€ìƒ‰ì–´\"}}, \"required\": [\"location\"]}}\\n\\n\\nThink very carefully before calling functions.\\nIf a you choose to call a function ONLY reply in the following format:\\n<{start_tag}={function_name}>{parameters}{end_tag}\\nwhere\\n\\nstart_tag => `<function`\\nparameters => a JSON dict with the function argument name as key and function argument value as value.\\nend_tag => `</function>`\\n\\nHere is an example,\\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\\n\\nReminder:\\n- If looking for real time information use relevant functions before falling back to brave_search\\n- Function calls MUST follow the specified format, start with <function= and end with </function>\\n- Required parameters MUST be specified\\n- Only call one function at a time\\n- Put the entire function call reply on one line\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nìˆ˜ë„ìš”ê¸ˆì€ ì–´ë””ì— ë¬¸ì˜í•´?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nìˆ˜ë„ìš”ê¸ˆì€ ì§€ì—­ ìˆ˜ë„ì‚¬ì—…ì†Œë‚˜ ì‹œì²­ ê´€ë ¨ ë¶€ì„œì— ë¬¸ì˜í•˜ë©´ ë¼ìš”. ê³ ì§€ì„œì— ì íŒ ì „í™”ë²ˆí˜¸ë¡œ ì—°ë½í•˜ë©´ ê°€ì¥ ë¹ ë¥´ê²Œ ì•ˆë‚´ë°›ì„ ìˆ˜ ìˆì–´ìš”.<|eot_id|>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# assistant_only_loss = Falseì¼ ê²½ìš°\n",
    "def formatting_prompts_func(examples):\n",
    "    \n",
    "    texts = []\n",
    "\n",
    "    for message in examples['messages']:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            tools=TOOL_DEFINITIONS\n",
    "        )\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "    return {'text': texts}\n",
    "\n",
    "formatted_dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "print(formatted_dataset[0])\n",
    "\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.1)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "eval_ds = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a6f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í° ê°œìˆ˜: 489\n"
     ]
    }
   ],
   "source": [
    "sample_text = formatted_dataset[0][\"text\"]\n",
    "\n",
    "tokens = tokenizer(\n",
    "    sample_text,\n",
    "    add_special_tokens=False,\n",
    "    return_attention_mask=False,\n",
    "    return_tensors=None\n",
    ")\n",
    "\n",
    "print(\"í† í° ê°œìˆ˜:\", len(tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d17f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assistant_only_loss = Trueì¼ ê²½ìš°\n",
    "# def formatting_prompts_func(examples):\n",
    "#     if len(examples[\"messages\"]) > 0 and isinstance(examples[\"messages\"][0], list):\n",
    "#         messages_list = examples[\"messages\"] # ì˜ˆ) {\"messages\": [ [ì±„íŒ…1], [ì±„íŒ…2], [ì±„íŒ…3] ]}\n",
    "#     else:\n",
    "#         # ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬ (SFTTrainer ê²€ì¦ìš©) \n",
    "#         messages_list = [examples[\"messages\"]] # ì˜ˆ) {\"messages\": [ì±„íŒ…1]}\n",
    "\n",
    "#     texts = []\n",
    "#     for message in messages_list:\n",
    "#         text = tokenizer.apply_chat_template(\n",
    "#             message,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=False,\n",
    "#             tools=TOOL_DEFINITIONS\n",
    "#         )\n",
    "#         texts.append(text)\n",
    "\n",
    "#     # SFTTrainerì˜ formatting_funcëŠ” ë°˜ë“œì‹œ 'ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸'ë¥¼ ë°˜í™˜í•´ì•¼ í•œë‹¤.\n",
    "#     return texts\n",
    "\n",
    "# split_dataset2 = dataset.train_test_split(test_size=0.1)\n",
    "# train_ds2 = split_dataset2[\"train\"]\n",
    "# eval_ds2 = split_dataset2[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e3a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5014/5014 [00:04<00:00, 1081.47 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558/558 [00:01<00:00, 455.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # max_steps = 400,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate = 5e-5, \n",
    "        bf16 = True,\n",
    "        seed = 1234,\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        output_dir = \"outputs\",\n",
    "\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        # SFTConfigì˜ íŒŒë¼ë¯¸í„°\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        # model_init_kwargs={\"trust_remote_code\": True},\n",
    "        # assistant_only_loss=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    processing_class=tokenizer,\n",
    "    # formatting_func=formatting_prompts_func,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4693ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,014 | Num Epochs = 1 | Total steps = 627\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,228,864 (0.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='627' max='627' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [627/627 2:23:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.707200</td>\n",
       "      <td>0.262919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.225551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.215073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.209296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>0.205749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.204133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=627, training_loss=0.2952723731264543, metrics={'train_runtime': 8640.5938, 'train_samples_per_second': 0.58, 'train_steps_per_second': 0.073, 'total_flos': 1.130779972608e+17, 'train_loss': 0.2952723731264543, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851ae62",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3023beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch, psutil, os, gc, re, json\n",
    "from datetime import datetime, timedelta\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import GenerationConfig, TextIteratorStreamer\n",
    "from peft import PeftModel\n",
    "from threading import Thread\n",
    "from memory_monitor import MemoryMonitor\n",
    "import requests\n",
    "from MyModule import get_current_time, get_current_date, get_weather, search_address, get_alarms, set_alarms, delete_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: GPU monitoring not available: No module named 'pynvml'\n",
      "[Before Loading] RAM: 977.14 MB | VRAM: 0.00 MB\n",
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[After Loading] RAM: 1333.55 MB | VRAM: 0.00 MB\n",
      "[After Generating] RAM: 1695.93 MB | VRAM: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "monitor = MemoryMonitor(device_index=0)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "monitor.print_usage(label=\"Before Loading\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"../model/kanana-1.5-8b-instruct-2505\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./outputs/checkpoint-600\")\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "monitor.print_usage(label=\"After Loading\")\n",
    "\n",
    "# 1 token ìƒì„±\n",
    "prompt = \"ì•ˆë…•í•˜ì„¸ìš”.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "_ = model.generate(**inputs, max_new_tokens=1)\n",
    "monitor.print_usage(label=\"After Generating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d677cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "    \"get_alarms\": get_alarms,\n",
    "    \"set_alarms\": set_alarms,\n",
    "    \"delete_alarm\": delete_alarms,\n",
    "}\n",
    "\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_alarms\",\n",
    "        \"description\": \"í˜„ì¬ ë“±ë¡ëœ ëª¨ë“  ì•ŒëŒ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤. (ë§¤ê°œë³€ìˆ˜ ì—†ìŒ)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"set_alarms\",\n",
    "        \"description\": \"ìƒˆë¡œìš´ ì•ŒëŒì„ ì˜ˆì•½í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"message\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì•ŒëŒ ì‹œ ì•Œë ¤ì¤„ ë‚´ìš©\"\n",
    "                },\n",
    "                \"year\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"\n",
    "                },\n",
    "                \"month\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì›” (1-12)\"\n",
    "                },\n",
    "                \"day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì¼ (1-31)\"\n",
    "                },\n",
    "                \"hour\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì‹œ (0-23)\"\n",
    "                },\n",
    "                \"minute\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ë¶„ (0-59)\"\n",
    "                },\n",
    "                \"relative_day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"\n",
    "                },\n",
    "                \"week_offset\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"\n",
    "                },\n",
    "                \"day_of_week\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ìš”ì¼ (ì˜ˆ: 'ì›”ìš”ì¼', 'í™”ìš”ì¼', ..., 'ì¼ìš”ì¼')\"\n",
    "                },\n",
    "                \"is_ampm_specified\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"ì‚¬ìš©ìê°€ 'ì˜¤ì „', 'ì˜¤í›„', 'ì•„ì¹¨', 'ë°¤' ë“± ì‹œê°„ì„ ëª…í™•íˆ ì§€ì •í–ˆëŠ”ì§€ ì—¬ë¶€\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"hour\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"delete_alarms\",\n",
    "        \"description\": \"ë“±ë¡ëœ ì•ŒëŒì„ ì‚­ì œí•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"message\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì‚­ì œí•  ì•ŒëŒì˜ ë©”ì‹œì§€ ë‚´ìš©\"\n",
    "                },\n",
    "                \"year\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"\n",
    "                },\n",
    "                \"month\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì›” (1-12)\"\n",
    "                },\n",
    "                \"day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì¼ (1-31)\"\n",
    "                },\n",
    "                \"hour\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì‹œ (0-23)\"\n",
    "                },\n",
    "                \"minute\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ë¶„ (0-59)\"\n",
    "                },\n",
    "                \"relative_day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"\n",
    "                },\n",
    "                \"week_offset\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"\n",
    "                },\n",
    "                \"day_of_week\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ìš”ì¼ (ì˜ˆ: 'ì›”ìš”ì¼', 'í™”ìš”ì¼', ..., 'ì¼ìš”ì¼')\"\n",
    "                },\n",
    "                \"is_ampm_specified\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"ì‚¬ìš©ìê°€ 'ì˜¤ì „', 'ì˜¤í›„', 'ì•„ì¹¨', 'ë°¤' ë“± ì‹œê°„ì„ ëª…í™•íˆ ì§€ì •í–ˆëŠ”ì§€ ì—¬ë¶€\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9c9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=2048,  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,  # ë í† í° ëª…ì‹œ\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "def stream_with_langchain(question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        tools=TOOL_DEFINITIONS\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "        skip_prompt=True            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "    )           \n",
    "    \n",
    "    # Threadê°€ ìˆì–´ì•¼ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ë°±ê·¸ë¼ìš´ë“œë¡œ generate()ê°€ ì‹¤í–‰ë˜ê³ \n",
    "    # ì„œë¸Œ ìŠ¤ë ˆë“œì—ì„œ streamerë¡œ í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì½ì„ ìˆ˜ ìˆìŒ\n",
    "    # ë”°ë¼ì„œ ë™ì‹œì— ë³‘ë ¬ë¡œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "\n",
    "    thread = Thread(\n",
    "        target=model.generate, \n",
    "        kwargs=dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    )\n",
    "    thread.start()\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "    \n",
    "    thread.join()\n",
    "    print()     # ì¤„ë°”ê¿ˆ\n",
    "\n",
    "    pattern = r\"<function=(\\w+)>(.+?)</function>\"\n",
    "    match = re.search(pattern, full_response)\n",
    "\n",
    "    if match:\n",
    "        # í•¨ìˆ˜ í˜¸ì¶œ í˜•íƒœ ê°ì§€ë¨\n",
    "        func_name = match.group(1)  # get_weather\n",
    "        args_str = match.group(2) # \"location\": \"Seoul\"\n",
    "\n",
    "        args = json.loads(args_str)\n",
    "\n",
    "        tool_result = TOOLS[func_name](**args)\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps(tool_result, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "        formatted_text_2 = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        inputs2 = tokenizer(\n",
    "            formatted_text_2,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "        )\n",
    "\n",
    "        streamer2 = TextIteratorStreamer(\n",
    "            tokenizer,\n",
    "            skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "            skip_prompt=True            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "        )\n",
    "\n",
    "        thread2 = Thread(\n",
    "            target=model.generate, \n",
    "            kwargs=dict(\n",
    "                input_ids=inputs2[\"input_ids\"].to(model.device),\n",
    "                attention_mask=inputs2[\"attention_mask\"].to(model.device),\n",
    "                generation_config=generation_config,\n",
    "                streamer=streamer2,\n",
    "            )\n",
    "        )\n",
    "        thread2.start()\n",
    "        \n",
    "        final_response = \"\"\n",
    "        for text in streamer2:\n",
    "            print(text, end=\"\", flush=True)\n",
    "            final_response += text\n",
    "        \n",
    "        thread2.join()\n",
    "        print()\n",
    "\n",
    "        return final_response\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5b4fe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ë‹¬ì„œêµ¬ ë‚ ì”¨ ì–´ë•Œ? ë¹„ì˜¤ë ¤ë‚˜? ìš°ì‚°ì„ ì•ˆ ì±™ê²¼ëŠ”ë° ê±±ì •ì´ë„¤\n",
      "A: <function=get_weather>{\"city\":\"ë‹¬ì„œêµ¬\"}</function>\n",
      "\n",
      "</function>\n",
      "{'city': 'ëŒ€êµ¬ ë‹¬ì„œêµ¬', 'temperature': '4ë„', 'sky_condition': 'íë¦¼', 'precipitation_type': 'ì—†ìŒ'}\n",
      "í˜„ì¬ ëŒ€êµ¬ ë‹¬ì„œêµ¬ëŠ” 4ë„ì´ë©° íë¦° ë‚ ì”¨ì…ë‹ˆë‹¤. ë¹„ ì†Œì‹ì€ ì—†ì§€ë§Œ, ê°‘ìê¸° íë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ìš°ì‚°ì„ ì±™ê¸°ëŠ” ê²Œ ì¢‹ê² ì–´ìš”.\n"
     ]
    }
   ],
   "source": [
    "question = \"ë‹¬ì„œêµ¬ ë‚ ì”¨ ì–´ë•Œ? ë¹„ì˜¤ë ¤ë‚˜? ìš°ì‚°ì„ ì•ˆ ì±™ê²¼ëŠ”ë° ê±±ì •ì´ë„¤\"\n",
    "print(f\"Q: {question}\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd103b3d",
   "metadata": {},
   "source": [
    "### GGUF ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5f8fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678f22f16149424f982c80a095a77fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.3 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected local model directory: /home/user/work/model/kanana-1.5-8b-instruct-2505\n",
      "Found HuggingFace hub cache directory: /home/user/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [01:51<05:33, 111.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00003-of-00004.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [03:42<03:42, 111.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00002-of-00004.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [05:23<01:46, 106.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00001-of-00004.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [05:53<00:00, 88.30s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00004-of-00004.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [06:49<00:00, 102.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/user/work/kanana_merged_temp`\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ëª¨ë¸ ë° ì•„ë‹µí„° ë¡œë“œ (í•œ ë²ˆì— ë¶ˆëŸ¬ì˜¤ê¸°)\n",
    "adapter_path = \"./outputs/checkpoint-627\" # í•™ìŠµëœ ì•„ë‹µí„°\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = adapter_path, # ì•„ë‹µí„° ê²½ë¡œë¥¼ ë„£ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ê¹Œì§€ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ í•©ì¹œë‹¤.\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë“œë¡œ ë³€í™˜ (í•„ìˆ˜)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# ì ¯ìŠ¨ìš© GGUFë¡œ ì €ì¥í•˜ê¸°\n",
    "model.save_pretrained_merged(\n",
    "    \"../kanana_merged_temp\",    # ì €ì¥ë  í´ë”ëª…/íŒŒì¼ëª…\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e40bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/work/llama.cpp\n",
      "INFO:hf-to-gguf:Loading model: kanana_merged_temp\n",
      "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:heuristics detected bfloat16 tensor dtype, setting --outtype bf16\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128259}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128259}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 8000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "The tokenizer you are loading from '../kanana_merged_temp' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128001\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_sep_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {# version=v3-llama3.1 #}{%- macro append_new_param_info(param_declaration, comment_info, examples_info, depth) -%}\n",
      "    {%- set offset = \"\" -%}\n",
      "    {%- if depth >= 1 -%}\n",
      "        {%- set offset = \"    \" * depth -%}\n",
      "    {%- endif -%}\n",
      "    {%- if comment_info != \"<|NONE|>\" -%}\n",
      "        {{ \"\\n\" + offset + comment_info }}\n",
      "        {%- if examples_info | length > 0 -%}\n",
      "            {# Append each example info #}\n",
      "            {%- for example in examples_info -%}\n",
      "                {{ \"\\n\" + offset + \"// \" + example|string|replace(\"'\", '\"') }}\n",
      "            {%- endfor -%}\n",
      "        {%- endif -%}\n",
      "    {%- endif -%}\n",
      "    {{ \"\\n\" + offset + param_declaration }}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro convert_data_type(param_type) -%}\n",
      "    {%- if param_type == \"integer\" or param_type == \"float\" -%}\n",
      "        {{ \"number\" }}\n",
      "    {%- else -%}\n",
      "        {{ param_type }}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_param_type(param) -%}\n",
      "    {%- set param_type = \"any\" -%}\n",
      "\n",
      "    {%- if \"type\" in param -%}\n",
      "        {%- set raw_param_type = param[\"type\"] -%}\n",
      "        {%- if raw_param_type is iterable and raw_param_type is not string -%}\n",
      "            {%- set param_type = raw_param_type | join(\" | \") -%}\n",
      "        {%- else -%}\n",
      "            {%- set param_type = raw_param_type -%}\n",
      "        {%- endif -%}\n",
      "        {{ convert_data_type(param_type) }}\n",
      "    {%- elif \"oneOf\" in param -%}\n",
      "        {%- set one_of_types = param[\"oneOf\"]|selectattr(\"type\", \"defined\")|list -%}\n",
      "        {%- set one_of_types = one_of_types|map(attribute=\"type\")|unique|list -%}\n",
      "        {{ convert_data_type(one_of_types | join(\" | \")) }}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_format_param(param) -%}\n",
      "    {%- if \"format\" in param -%}\n",
      "        {{ param[\"format\"] }}\n",
      "    {%- elif \"oneOf\" in param -%}\n",
      "        {%- set formats = [] -%}\n",
      "        {%- for item in param[\"oneOf\"] -%}\n",
      "            {%- if \"format\" in item -%}\n",
      "                {%- if item[\"format\"] == param[\"oneOf\"][-1][\"format\"] -%}\n",
      "                    {{ item[\"format\"] }}\n",
      "                {%- else -%}\n",
      "                    {{ item[\"format\"] + \" or \"}}\n",
      "                {%- endif -%}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ \"<|NONE|>\" }}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_param_info(param) -%}\n",
      "    {%- set param_type = param.get(\"type\", \"any\") -%}\n",
      "    {%- set format_param = get_format_param(param) -%}\n",
      "\n",
      "    {%- if \"description\" in param or \"default\" in param or format_param != \"<|NONE|>\" or param[\"maximum\"] or param[\"minimum\"] or param[\"maxLength\"] or param[\"minLength\"] -%}\n",
      "        {{ \"//\" }}\n",
      "        {%- if \"description\" in param -%}\n",
      "            {%- set desc = param[\"description\"] -%}\n",
      "            {%- if not desc.endswith(\".\") -%}\n",
      "                {%- set desc = desc + \".\" -%}\n",
      "            {%- endif -%}\n",
      "            {{ \" \" + desc }}\n",
      "        {%- endif -%}\n",
      "\n",
      "        {%- if \"default\" in param -%}\n",
      "            {%- set default_value = param[\"default\"] -%}\n",
      "            {%- if param_type == \"string\" -%}\n",
      "                {%- set default_value = '\"' ~ default_value ~ '\"' -%}\n",
      "            {%- endif -%}\n",
      "            {{ \" Default=\" ~ default_value ~ \".\" }}\n",
      "        {%- endif -%}\n",
      "\n",
      "        {%- set format_param = get_format_param(param) -%}\n",
      "        {%- if format_param != \"<|NONE|>\" -%}\n",
      "            {{ \" Format=\" ~ format_param }}\n",
      "        {%- endif -%}\n",
      "\n",
      "        {%- for field, field_name in [(\"maximum\", \"Maximum\"), (\"minimum\", \"Minimum\"), (\"maxLength\", \"Maximum length\"), (\"minLength\", \"Minimum length\")] -%}\n",
      "            {%- if field in param -%}\n",
      "                {{ \" \" + field_name ~ \"=\" ~ param[field] }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ \"<|NONE|>\"}}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_enum_option_str(enum_options) -%}\n",
      "    {%- for v in enum_options -%}\n",
      "        {%- if v is string -%}\n",
      "            {{ '\"' + v + '\"' }}\n",
      "        {%- else -%}\n",
      "            {{ v }}\n",
      "        {%- endif -%}\n",
      "        {%- if enum_options|length > 0 and v != enum_options[-1] -%}\n",
      "            {{ \" | \" }}\n",
      "        {%- endif -%}\n",
      "    {%- endfor -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_array_typescript(param_name, param_dic, depth) -%}\n",
      "    {%- set offset = '' -%}\n",
      "    {%- if depth >= 1 -%}\n",
      "        {%- set offset = \"    \" * depth -%}\n",
      "    {%- endif -%}\n",
      "    {%- set items_info = param_dic.get('items', {}) -%}\n",
      "\n",
      "    {%- if items_info|length == 0 -%}\n",
      "        {%- if param_name -%}\n",
      "            {{ \"\\n\" + offset + param_name + \": []\" }}\n",
      "        {%- else -%}\n",
      "            {{ \"\\n\" + offset + \"[]\" }}\n",
      "        {%- endif -%}\n",
      "    {%- else -%}\n",
      "        {%- set array_type = get_param_type(items_info) -%}\n",
      "        {%- if array_type == 'object' -%}\n",
      "            {%- if param_name -%}\n",
      "                {{ \"\\n\" + offset + param_name + \": {\" }}\n",
      "            {%- else -%}\n",
      "                {{ \"\\n\" + offset + \"{\" }}\n",
      "            {%- endif -%}\n",
      "            {{ get_parameter_typescript(items_info.get('properties', {}), items_info.get('required', []), depth + 1) -}}\n",
      "            {{- \"\\n\" + offset + \"}[]\" }}\n",
      "        {%- elif array_type == 'array' -%}\n",
      "            {%- set item_info = get_array_typescript(None, items_info, depth + 1) -%}\n",
      "            {%- if not param_name -%}\n",
      "                {{ \"\\n\" + item_info + \"[]\" }}\n",
      "            {%- else -%}\n",
      "                {{ \"\\n\" + offset + param_name + \": \" + item_info|trim + \"[]\" }}\n",
      "            {%- endif -%}\n",
      "        {%- else -%}\n",
      "            {%- if 'enum' in items_info -%}\n",
      "                {%- set item_type = get_enum_option_str(items_info['enum']) -%}\n",
      "                {%- if param_name is none -%}\n",
      "                    {{ \"(\" + item_type + \")[]\"}}\n",
      "                {%- else -%}\n",
      "                    {{ \"\\n\" + offset + param_name + \": (\" + item_type + \")[]\" }}\n",
      "                {%- endif -%}\n",
      "            {%- else -%}\n",
      "                {%- if param_name is none -%}\n",
      "                    {{ \"\\n\" + array_type + \"[]\" }}\n",
      "                {%- else -%}\n",
      "                    {{ \"\\n\" + offset + param_name + \": \" + array_type + \"[],\" }}\n",
      "                {%- endif -%}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_parameter_typescript(properties, required_params, depth=0) -%}\n",
      "    {%- set res = \"\" -%}\n",
      "    {%- for param_name, param in properties.items() -%}\n",
      "        {%- if param is mapping -%}\n",
      "            {%- set comment_info = get_param_info(param) -%}\n",
      "            {# Param Examples #}\n",
      "            {%- set examples_info = [] -%}\n",
      "            {%- if \"examples\" in param -%}\n",
      "                {%- set examples_info = [\"Example \" + param_name + \":\"] -%}\n",
      "                {%- set examples_info = examples_info + param[\"examples\"] -%}\n",
      "            {%- endif -%}\n",
      "\n",
      "            {# Param Name declaration #}\n",
      "            {%- set param_declaration = param_name -%}\n",
      "            {%- if required_params is iterable and param_name not in required_params -%}\n",
      "                {%- set param_declaration = param_declaration + \"?\" -%}\n",
      "            {%- endif -%}\n",
      "\n",
      "            {%- set param_type = get_param_type(param) -%}\n",
      "\n",
      "            {# Handle indentation based on depth #}\n",
      "            {%- set offset = \"\" -%}\n",
      "            {%- if depth >= 1 -%}\n",
      "                {%- set offset = \"    \" * depth -%}\n",
      "            {%- endif -%}\n",
      "\n",
      "            {%- if param_type == \"object\" -%}\n",
      "                {%- if comment_info != \"<|NONE|>\" -%}\n",
      "                    {{ \"\\n\" + offset + comment_info }}\n",
      "                {%- endif -%}\n",
      "                {%- if examples_info|length > 0 -%}\n",
      "                    {%- for example in examples_info -%}\n",
      "                        {{ \"\\n\" + offset + \"// \" + example|string|replace(\"'\", '\"') }}\n",
      "                    {%- endfor -%}\n",
      "                {%- endif -%}\n",
      "                {%- set param_declaration = param_declaration + \": {\" -%}\n",
      "                {{ \"\\n\" + offset + param_declaration -}}\n",
      "                {{- get_parameter_typescript(param.get(\"properties\", {}), param.get(\"required\", []), depth + 1) -}}\n",
      "                {{- \"\\n\" + offset + \"},\" }}\n",
      "            {%- elif param_type == \"array\" -%}\n",
      "                {%- set item_info = param.get(\"items\", {}) -%}\n",
      "                {%- if \"type\" not in item_info -%}\n",
      "                    {%- set param_declaration = param_declaration + \": [],\" -%}\n",
      "                    {{ append_new_param_info(param_declaration, comment_info, examples_info, depth) }}\n",
      "                {%- else -%}\n",
      "                    {%- if comment_info != \"<|NONE|>\" -%}\n",
      "                        {{ \"\\n\" + offset + comment_info }}\n",
      "                    {%- endif -%}\n",
      "                    {%- if examples_info|length > 0 -%}\n",
      "                        {%- for example in examples_info -%}\n",
      "                            {{ \"\\n\" + offset + \"// \" + example|string|replace(\"'\", '\"') }}\n",
      "                        {%- endfor -%}\n",
      "                    {%- endif -%}\n",
      "                    {%- set array_declaration = get_array_typescript(param_declaration, param, depth) -%}\n",
      "                    {%- if not array_declaration.endswith(\",\") -%}\n",
      "                        {%- set array_declaration = array_declaration + \",\" -%}\n",
      "                    {%- endif -%}\n",
      "                    {{ array_declaration}}\n",
      "                {%- endif -%}\n",
      "            {%- else -%}\n",
      "                {%- if \"enum\" in param -%}\n",
      "                    {%- set param_type = get_enum_option_str(param[\"enum\"]) -%}\n",
      "                {%- endif -%}\n",
      "                {%- if \"nullable\" in param and param[\"nullable\"] -%}\n",
      "                    {%- set param_type = param_type + \" | null\" -%}\n",
      "                {%- endif -%}\n",
      "                {%- set param_declaration = param_declaration + \": \" + param_type + \",\" -%}\n",
      "                {{ append_new_param_info(param_declaration, comment_info, examples_info, depth) }}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- endfor -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro generate_schema_from_functions(functions, namespace='functions') -%}\n",
      "    {{ \"// Supported function definitions that should be called when necessary.\\n\" -}}\n",
      "    {{- \"namespace \" + namespace + \" {\\n\\n\" -}}\n",
      "\n",
      "    {%- for function in functions -%}\n",
      "        {%- if function.get(\"function\") -%}\n",
      "            {%- set function = function.get(\"function\") -%}\n",
      "        {%- endif -%}\n",
      "\n",
      "        {%- set function_name = function.get(\"name\") -%}\n",
      "        {%- if function_name -%}\n",
      "            {%- set description = function.get('description', '') -%}\n",
      "            {%- set parameters = function.get('parameters', {}) -%}\n",
      "            {{- \"// \" + description + \"\\n\" -}}\n",
      "            {{- \"type \" + function_name -}}\n",
      "            {%- if parameters and parameters.get(\"properties\") -%}\n",
      "                {{- \" = (_: {\" -}}\n",
      "                {%- set required_params = parameters.get(\"required\", []) -%}\n",
      "                {{ get_parameter_typescript(parameters.get(\"properties\"), required_params, 0) -}}\n",
      "                {{- \"\\n}) => any;\\n\\n\" }}\n",
      "            {%- else -%}\n",
      "                {{ \" = () => any;\\n\\n\" }}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- endfor -%}\n",
      "    {{ \"} // namespace \" + namespace }}\n",
      "{%- endmacro -%}\n",
      "{%- if not tools is defined -%}\n",
      "    {%- set tools = none -%}\n",
      "{%- endif -%}\n",
      "\n",
      "{%- set has_code_interpreter = tools | selectattr(\"type\", \"equalto\", \"code_interpreter\") | list | length > 0 -%}\n",
      "{%- if has_code_interpreter -%}\n",
      "    {%- set tools = tools | rejectattr(\"type\", \"equalto\", \"code_interpreter\") | list -%}\n",
      "{%- endif -%}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- bos_token + \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if has_code_interpreter %}\n",
      "    {{- \"Environment: ipython\\n\\n\" }}\n",
      "{%- else -%}\n",
      "    {{ \"\\n\"}}\n",
      "{%- endif %}\n",
      "{%- if tools %}\n",
      "    {{- \"\\nYou have access to the following functions:\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {%- if \"type\" in t -%}\n",
      "            {{ \"Use the function '\" + t[\"function\"][\"name\"] + \"' to '\" + t[\"function\"][\"description\"] + \"'\\n\" + t[\"function\"] | tojson() }}\n",
      "        {%- else -%}\n",
      "            {{ \"Use the function '\" + t[\"name\"] + \"' to '\" + t[\"description\"] + \"'\\n\" + t | tojson }}\n",
      "        {%- endif -%}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- '\\nThink very carefully before calling functions.\\nIf a you choose to call a function ONLY reply in the following format:\\n<{start_tag}={function_name}>{parameters}{end_tag}\\nwhere\\n\\nstart_tag => `<function`\\nparameters => a JSON dict with the function argument name as key and function argument value as value.\\nend_tag => `</function>`\\n\\nHere is an example,\\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\\n\\nReminder:\\n- If looking for real time information use relevant functions before falling back to brave_search\\n- Function calls MUST follow the specified format, start with <function= and end with </function>\\n- Required parameters MUST be specified\\n- Only call one function at a time\\n- Put the entire function call reply on one line\\n\\n' -}}\n",
      "{%- endif %}\n",
      "{{- \"<|eot_id|>\" -}}\n",
      "\n",
      "{%- for message in messages -%}\n",
      "    {%- if message['role'] == 'user' or message['role'] == 'system' -%}\n",
      "        {{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}\n",
      "    {%- elif message['role'] == 'tool' -%}\n",
      "        {{ '<|start_header_id|>ipython<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}\n",
      "    {%- else -%}\n",
      "        {%- if (message['content'] and message['content']|length > 0) or ('tool_calls' in message and message['tool_calls'] and message['tool_calls']|length > 0) -%}\n",
      "            {{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'}}\n",
      "        {%- endif -%}\n",
      "        {%- if message['content'] and message['content']|length > 0 -%}\n",
      "            {{ message['content'] }}\n",
      "        {%- endif -%}\n",
      "        {%- if 'tool_calls' in message and message['tool_calls'] and message['tool_calls']|length > 0 -%}\n",
      "            {%- for tool_call in message['tool_calls'] -%}\n",
      "                {%- if tool_call[\"function\"][\"name\"] == \"python\" -%}\n",
      "                    {{ '<|python_tag|>' + tool_call['function']['arguments'] }}\n",
      "                {%- else -%}\n",
      "                    {{ '<function=' + tool_call['function']['name'] + '>' + tool_call['function']['arguments'] + '</function>' }}\n",
      "                {%- endif -%}\n",
      "            {%- endfor -%}\n",
      "            {{ '<|eom_id|>' }}\n",
      "        {%- elif message['content'] and message['content']|length > 0 -%}\n",
      "            {{ '<|eot_id|>' }}\n",
      "        {%- endif -%}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif -%}\n",
      "\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:../kanana_f16.gguf: n_tensors = 291, total_size = 16.1G\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.1G/16.1G [08:04<00:00, 33.1Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to ../kanana_f16.gguf\n",
      "main: build = 7708 (0c3b7a9ef)\n",
      "main: built with GNU 13.3.0 for Linux x86_64\n",
      "main: quantizing '../kanana_f16.gguf' to '../kanana_q4_k_m.gguf' as Q4_K_M\n",
      "llama_model_loader: direct I/O is enabled, disabling mmap\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from ../kanana_f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Kanana_Merged_Temp\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 8000000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128259\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128259]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128259]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_sep_token bool             = false\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {# version=v3-llama3.1 #}{%- macro ap...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type bf16:  226 tensors\n",
      "[   1/ 291]                        output.weight - [ 4096, 128259,     1,     1], type =   bf16, converting to q6_K .. size =  1002.02 MiB ->   410.99 MiB\n",
      "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[   3/ 291]                    token_embd.weight - [ 4096, 128259,     1,     1], type =   bf16, converting to q4_K .. size =  1002.02 MiB ->   281.82 MiB\n",
      "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
      "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "llama_model_quantize_impl: model size  = 15317.06 MiB\n",
      "llama_model_quantize_impl: quant size  =  4685.32 MiB\n",
      "\n",
      "main: quantize time = 320377.27 ms\n",
      "main:    total time = 320377.27 ms\n"
     ]
    }
   ],
   "source": [
    "# 1. llama.cpp í´ë”ë¡œ ì´ë™ (ì£¼í”¼í„° ì„¸ì…˜ ë‚´ ê²½ë¡œ ë³€ê²½ì€ %cd ì‚¬ìš©)\n",
    "%cd /home/user/work/llama.cpp\n",
    "\n",
    "# 2. f16 ggufë¡œ ë³€í™˜ (!ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…¸ ëª…ë ¹ì–´ ì‹¤í–‰)\n",
    "!python convert_hf_to_gguf.py ../kanana_merged_temp --outfile ../kanana_f16.gguf\n",
    "\n",
    "# 3. q4_k_m ggufë¡œ ë³€í™˜\n",
    "!./build/bin/llama-quantize ../kanana_f16.gguf ../kanana_q4_k_m.gguf q4_k_m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
