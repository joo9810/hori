{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be328cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch, psutil, os, gc\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from MyModule import get_current_time, get_current_date, get_weather, search_address, get_alarms, set_alarms, delete_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8df172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"../model/kanana-1.5-2.1b-instruct-2505\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93aeadbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2026.1.3 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb461cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 370 examples [00:00, 4355.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user', 'content': 'ì´ë²ˆì£¼ ì¼ìš”ì¼ ì•„ì¹¨ 9ì‹œ êµíšŒ ê°€ê¸°'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '<function=set_alarms>{\"message\": \"êµíšŒ ê°€ê¸°\", \"year\": null, \"month\": null, \"day\": null, \"hour\": 9, \"minute\": 0, \"relative_day\": 0, \"week_offset\": 0, \"day_of_week\": \"ì¼ìš”ì¼\", \"is_ampm_specified\": true}</function>'},\n",
       "  {'role': 'tool',\n",
       "   'content': '{\"status\": \"success\", \"time\": \"20260125T0900\", \"day_of_week\": \"ì¼ìš”ì¼\", \"message_content\": \"êµíšŒ ê°€ê¸°\", \"message\": \"ì•ŒëŒì„ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\"} '},\n",
       "  {'role': 'assistant', 'content': \"1ì›” 25ì¼ ì¼ìš”ì¼ ì˜¤ì „ 9ì‹œì— 'êµíšŒ ê°€ê¸°' ì•ŒëŒì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\"}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = load_dataset(\"json\", data_files=\"./data/train_data_singleturn.jsonl\")\n",
    "dataset2 = load_dataset(\"json\", data_files=\"./data/train_data_function_call_kanana.jsonl\")\n",
    "dataset3 = load_dataset(\"json\", data_files=\"./data/train_data_function_call_alarm.jsonl\")\n",
    "\n",
    "dataset = concatenate_datasets([\n",
    "    dataset1['train'],\n",
    "    dataset2['train'],\n",
    "    dataset3['train'],\n",
    "    dataset3['train'],\n",
    "]).shuffle(seed=42)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3d92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "    \"get_alarms\": get_alarms,\n",
    "    \"set_alarms\": set_alarms,\n",
    "    \"delete_alarm\": delete_alarms,\n",
    "}\n",
    "\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_alarms\",\n",
    "        \"description\": \"í˜„ì¬ ë“±ë¡ëœ ëª¨ë“  ì•ŒëŒ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤. (ë§¤ê°œë³€ìˆ˜ ì—†ìŒ)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"set_alarms\",\n",
    "        \"description\": \"ìƒˆë¡œìš´ ì•ŒëŒì„ ì˜ˆì•½í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"message\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì•ŒëŒ ì‹œ ì•Œë ¤ì¤„ ë‚´ìš©\"\n",
    "                },\n",
    "                \"year\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"\n",
    "                },\n",
    "                \"month\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì›” (1-12)\"\n",
    "                },\n",
    "                \"day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì¼ (1-31)\"\n",
    "                },\n",
    "                \"hour\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì‹œ (0-23)\"\n",
    "                },\n",
    "                \"minute\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ë¶„ (0-59)\"\n",
    "                },\n",
    "                \"relative_day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"\n",
    "                },\n",
    "                \"week_offset\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"\n",
    "                },\n",
    "                \"day_of_week\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ìš”ì¼ (ì˜ˆ: 'ì›”ìš”ì¼', 'í™”ìš”ì¼', ..., 'ì¼ìš”ì¼')\"\n",
    "                },\n",
    "                \"is_ampm_specified\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"ì‚¬ìš©ìê°€ 'ì˜¤ì „', 'ì˜¤í›„', 'ì•„ì¹¨', 'ë°¤' ë“± ì‹œê°„ì„ ì§€ì •í–ˆë‹¤ë©´ True, ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´ Falseë¥¼ ì£¼ì„¸ìš”.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"hour\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"delete_alarms\",\n",
    "        \"description\": \"ë“±ë¡ëœ ì•ŒëŒì„ ì‚­ì œí•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"message\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì‚­ì œí•  ì•ŒëŒì˜ ë©”ì‹œì§€ ë‚´ìš©\"\n",
    "                },\n",
    "                \"year\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"\n",
    "                },\n",
    "                \"month\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì›” (1-12)\"\n",
    "                },\n",
    "                \"day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì¼ (1-31)\"\n",
    "                },\n",
    "                \"hour\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì‹œ (0-23)\"\n",
    "                },\n",
    "                \"minute\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ë¶„ (0-59)\"\n",
    "                },\n",
    "                \"relative_day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"\n",
    "                },\n",
    "                \"week_offset\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"\n",
    "                },\n",
    "                \"day_of_week\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ìš”ì¼ (ì˜ˆ: 'ì›”ìš”ì¼', 'í™”ìš”ì¼', ..., 'ì¼ìš”ì¼')\"\n",
    "                },\n",
    "                \"is_ampm_specified\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"ì‚¬ìš©ìê°€ 'ì˜¤ì „', 'ì˜¤í›„', 'ì•„ì¹¨', 'ë°¤' ë“± ì‹œê°„ì„ ì§€ì •í–ˆë‹¤ë©´ True, ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´ Falseë¥¼ ì£¼ì„¸ìš”.\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a41b4a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5982/5982 [00:00<00:00, 6677.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\\n\\nYou have access to the following functions:\\n\\nUse the function \\'get_current_time\\' to \\'í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\\'\\n{\"name\": \"get_current_time\", \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}\\n\\nUse the function \\'get_current_date\\' to \\'í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\\'\\n{\"name\": \"get_current_date\", \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}\\n\\nUse the function \\'get_weather\\' to \\'íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\\'\\n{\"name\": \"get_weather\", \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"ì§€ì—­ ì´ë¦„\"}}, \"required\": [\"city\"]}}\\n\\nUse the function \\'search_address\\' to \\'ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\\'\\n{\"name\": \"search_address\", \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"ê²€ìƒ‰ì–´\"}}, \"required\": [\"location\"]}}\\n\\nUse the function \\'get_alarms\\' to \\'í˜„ì¬ ë“±ë¡ëœ ëª¨ë“  ì•ŒëŒ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤. (ë§¤ê°œë³€ìˆ˜ ì—†ìŒ)\\'\\n{\"name\": \"get_alarms\", \"description\": \"í˜„ì¬ ë“±ë¡ëœ ëª¨ë“  ì•ŒëŒ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤. (ë§¤ê°œë³€ìˆ˜ ì—†ìŒ)\"}\\n\\nUse the function \\'set_alarms\\' to \\'ìƒˆë¡œìš´ ì•ŒëŒì„ ì˜ˆì•½í•©ë‹ˆë‹¤.\\'\\n{\"name\": \"set_alarms\", \"description\": \"ìƒˆë¡œìš´ ì•ŒëŒì„ ì˜ˆì•½í•©ë‹ˆë‹¤.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"message\": {\"type\": \"string\", \"description\": \"ì•ŒëŒ ì‹œ ì•Œë ¤ì¤„ ë‚´ìš©\"}, \"year\": {\"type\": \"integer\", \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"}, \"month\": {\"type\": \"integer\", \"description\": \"ì›” (1-12)\"}, \"day\": {\"type\": \"integer\", \"description\": \"ì¼ (1-31)\"}, \"hour\": {\"type\": \"integer\", \"description\": \"ì‹œ (0-23)\"}, \"minute\": {\"type\": \"integer\", \"description\": \"ë¶„ (0-59)\"}, \"relative_day\": {\"type\": \"integer\", \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"}, \"week_offset\": {\"type\": \"integer\", \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"}, \"day_of_week\": {\"type\": \"string\", \"description\": \"ìš”ì¼ (ì˜ˆ: \\'ì›”ìš”ì¼\\', \\'í™”ìš”ì¼\\', ..., \\'ì¼ìš”ì¼\\')\"}, \"is_ampm_specified\": {\"type\": \"boolean\", \"description\": \"ì‚¬ìš©ìê°€ \\'ì˜¤ì „\\', \\'ì˜¤í›„\\', \\'ì•„ì¹¨\\', \\'ë°¤\\' ë“± ì‹œê°„ì„ ì§€ì •í–ˆë‹¤ë©´ True, ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´ Falseë¥¼ ì£¼ì„¸ìš”.\"}}, \"required\": [\"hour\"]}}\\n\\nUse the function \\'delete_alarms\\' to \\'ë“±ë¡ëœ ì•ŒëŒì„ ì‚­ì œí•©ë‹ˆë‹¤\\'\\n{\"name\": \"delete_alarms\", \"description\": \"ë“±ë¡ëœ ì•ŒëŒì„ ì‚­ì œí•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {\"message\": {\"type\": \"string\", \"description\": \"ì‚­ì œí•  ì•ŒëŒì˜ ë©”ì‹œì§€ ë‚´ìš©\"}, \"year\": {\"type\": \"integer\", \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"}, \"month\": {\"type\": \"integer\", \"description\": \"ì›” (1-12)\"}, \"day\": {\"type\": \"integer\", \"description\": \"ì¼ (1-31)\"}, \"hour\": {\"type\": \"integer\", \"description\": \"ì‹œ (0-23)\"}, \"minute\": {\"type\": \"integer\", \"description\": \"ë¶„ (0-59)\"}, \"relative_day\": {\"type\": \"integer\", \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"}, \"week_offset\": {\"type\": \"integer\", \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"}, \"day_of_week\": {\"type\": \"string\", \"description\": \"ìš”ì¼ (ì˜ˆ: \\'ì›”ìš”ì¼\\', \\'í™”ìš”ì¼\\', ..., \\'ì¼ìš”ì¼\\')\"}, \"is_ampm_specified\": {\"type\": \"boolean\", \"description\": \"ì‚¬ìš©ìê°€ \\'ì˜¤ì „\\', \\'ì˜¤í›„\\', \\'ì•„ì¹¨\\', \\'ë°¤\\' ë“± ì‹œê°„ì„ ì§€ì •í–ˆë‹¤ë©´ True, ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´ Falseë¥¼ ì£¼ì„¸ìš”.\"}}}}\\n\\n\\nThink very carefully before calling functions.\\nIf a you choose to call a function ONLY reply in the following format:\\n<{start_tag}={function_name}>{parameters}{end_tag}\\nwhere\\n\\nstart_tag => `<function`\\nparameters => a JSON dict with the function argument name as key and function argument value as value.\\nend_tag => `</function>`\\n\\nHere is an example,\\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\\n\\nReminder:\\n- If looking for real time information use relevant functions before falling back to brave_search\\n- Function calls MUST follow the specified format, start with <function= and end with </function>\\n- Required parameters MUST be specified\\n- Only call one function at a time\\n- Put the entire function call reply on one line\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nì´ë²ˆì£¼ ì¼ìš”ì¼ ì•„ì¹¨ 9ì‹œ êµíšŒ ê°€ê¸°<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<function=set_alarms>{\"message\": \"êµíšŒ ê°€ê¸°\", \"year\": null, \"month\": null, \"day\": null, \"hour\": 9, \"minute\": 0, \"relative_day\": 0, \"week_offset\": 0, \"day_of_week\": \"ì¼ìš”ì¼\", \"is_ampm_specified\": true}</function><|eot_id|><|start_header_id|>ipython<|end_header_id|>\\n\\n{\"status\": \"success\", \"time\": \"20260125T0900\", \"day_of_week\": \"ì¼ìš”ì¼\", \"message_content\": \"êµíšŒ ê°€ê¸°\", \"message\": \"ì•ŒëŒì„ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\"} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n1ì›” 25ì¼ ì¼ìš”ì¼ ì˜¤ì „ 9ì‹œì— \\'êµíšŒ ê°€ê¸°\\' ì•ŒëŒì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "# assistant_only_loss = Falseì¼ ê²½ìš°\n",
    "def formatting_prompts_func(examples):\n",
    "    \n",
    "    texts = []\n",
    "\n",
    "    for message in examples['messages']:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            tools=TOOL_DEFINITIONS\n",
    "        )\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "    return {'text': texts}\n",
    "\n",
    "formatted_dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "print(formatted_dataset[0])\n",
    "\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.1)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "eval_ds = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a6f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í° ê°œìˆ˜: 1353\n"
     ]
    }
   ],
   "source": [
    "sample_text = formatted_dataset[0][\"text\"]\n",
    "\n",
    "tokens = tokenizer(\n",
    "    sample_text,\n",
    "    add_special_tokens=False,\n",
    "    return_attention_mask=False,\n",
    "    return_tensors=None\n",
    ")\n",
    "\n",
    "print(\"í† í° ê°œìˆ˜:\", len(tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d17f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assistant_only_loss = Trueì¼ ê²½ìš°\n",
    "# def formatting_prompts_func(examples):\n",
    "#     if len(examples[\"messages\"]) > 0 and isinstance(examples[\"messages\"][0], list):\n",
    "#         messages_list = examples[\"messages\"] # ì˜ˆ) {\"messages\": [ [ì±„íŒ…1], [ì±„íŒ…2], [ì±„íŒ…3] ]}\n",
    "#     else:\n",
    "#         # ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬ (SFTTrainer ê²€ì¦ìš©) \n",
    "#         messages_list = [examples[\"messages\"]] # ì˜ˆ) {\"messages\": [ì±„íŒ…1]}\n",
    "\n",
    "#     texts = []\n",
    "#     for message in messages_list:\n",
    "#         text = tokenizer.apply_chat_template(\n",
    "#             message,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=False,\n",
    "#             tools=TOOL_DEFINITIONS\n",
    "#         )\n",
    "#         texts.append(text)\n",
    "\n",
    "#     # SFTTrainerì˜ formatting_funcëŠ” ë°˜ë“œì‹œ 'ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸'ë¥¼ ë°˜í™˜í•´ì•¼ í•œë‹¤.\n",
    "#     return texts\n",
    "\n",
    "# split_dataset2 = dataset.train_test_split(test_size=0.1)\n",
    "# train_ds2 = split_dataset2[\"train\"]\n",
    "# eval_ds2 = split_dataset2[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e3a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5383/5383 [00:04<00:00, 1246.78 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 599/599 [00:00<00:00, -601.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # max_steps = 400,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate = 5e-5, \n",
    "        bf16 = True,\n",
    "        seed = 1234,\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        output_dir = \"outputs\",\n",
    "\n",
    "        logging_steps=200,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        # SFTConfigì˜ íŒŒë¼ë¯¸í„°\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        # model_init_kwargs={\"trust_remote_code\": True},\n",
    "        # assistant_only_loss=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    processing_class=tokenizer,\n",
    "    # formatting_func=formatting_prompts_func,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4693ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,383 | Num Epochs = 1 | Total steps = 673\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 23,003,136 of 2,109,987,840 (1.09% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='604' max='673' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [604/673 2:21:14 < 16:11, 0.07 it/s, Epoch 0.90/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.257500</td>\n",
       "      <td>0.101896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.093805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.089616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/hori/unsloth_compiled_cache/UnslothSFTTrainer.py:64\u001b[0m, in \u001b[0;36mprepare_for_training_mode.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfor_training(use_gradient_checkpointing\u001b[38;5;241m=\u001b[39muse_gc)\n\u001b[0;32m---> 64\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Restore previous mode when possible\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:330\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/work/hori/unsloth_compiled_cache/UnslothSFTTrainer.py:1218\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m-> 1218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:40\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m~/work/hori/unsloth_compiled_cache/UnslothSFTTrainer.py:1207\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m ):\n\u001b[0;32m-> 1207\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/models/_utils.py:1659\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1652\u001b[0m     name \u001b[38;5;241m=\u001b[39m inner_model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1654\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1655\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1656\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1657\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1658\u001b[0m     )\n\u001b[0;32m-> 1659\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m<string>:36\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/_compile.py:53\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive, wrapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1046\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:1545\u001b[0m, in \u001b[0;36mPeftModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m   1535\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m input_ids,\n\u001b[1;32m   1536\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1543\u001b[0m     )\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:311\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:1351\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1364\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1366\u001b[0m bsz, q_len, hd \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:1126\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/modeling_layers.py:93\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/_compile.py:53\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive, wrapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1046\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/utils/checkpoint.py:496\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, early_stop, *args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m         )\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    499\u001b[0m         function, preserve, context_fn, determinism_check, debug, early_stop, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    500\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/autograd/function.py:581\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth_zoo/gradient_checkpointing.py:498\u001b[0m, in \u001b[0;36mUnslothCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39m_requires_gradient: ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 498\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gpu_buffer: MAIN_STREAM\u001b[38;5;241m.\u001b[39mwait_stream(EXTRA_STREAM)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:742\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    741\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 742\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:627\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m K \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mview(bsz, q_len, n_kv_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    626\u001b[0m V \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mview(bsz, q_len, n_kv_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 627\u001b[0m seq_info \u001b[38;5;241m=\u001b[39m \u001b[43mget_packed_info_from_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m kv_seq_len \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/utils/packing.py:220\u001b[0m, in \u001b[0;36mget_packed_info_from_kwargs\u001b[0;34m(kwargs, device)\u001b[0m\n\u001b[1;32m    218\u001b[0m cu_seqlens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(lengths\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint32, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m    219\u001b[0m cu_seqlens[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 220\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m max_seqlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(lengths\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lengths, cu_seqlens, max_seqlen\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851ae62",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3023beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch, psutil, os, gc, re, json\n",
    "from datetime import datetime, timedelta\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import GenerationConfig, TextIteratorStreamer\n",
    "from peft import PeftModel\n",
    "from threading import Thread\n",
    "from memory_monitor import MemoryMonitor\n",
    "import requests\n",
    "from MyModule import get_current_time, get_current_date, get_weather, search_address, get_alarms, set_alarms, delete_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d0bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: GPU monitoring not available: No module named 'pynvml'\n",
      "[Before Loading] RAM: 977.14 MB | VRAM: 0.00 MB\n",
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[After Loading] RAM: 1333.55 MB | VRAM: 0.00 MB\n",
      "[After Generating] RAM: 1695.93 MB | VRAM: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "monitor = MemoryMonitor(device_index=0)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "monitor.print_usage(label=\"Before Loading\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"../model/kanana-1.5-2.1b-instruct-2505\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./outputs/checkpoint-600\")\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "monitor.print_usage(label=\"After Loading\")\n",
    "\n",
    "# 1 token ìƒì„±\n",
    "prompt = \"ì•ˆë…•í•˜ì„¸ìš”.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "_ = model.generate(**inputs, max_new_tokens=1)\n",
    "monitor.print_usage(label=\"After Generating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d677cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "    \"get_alarms\": get_alarms,\n",
    "    \"set_alarms\": set_alarms,\n",
    "    \"delete_alarm\": delete_alarms,\n",
    "}\n",
    "\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_alarms\",\n",
    "        \"description\": \"í˜„ì¬ ë“±ë¡ëœ ëª¨ë“  ì•ŒëŒ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤. (ë§¤ê°œë³€ìˆ˜ ì—†ìŒ)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"set_alarms\",\n",
    "        \"description\": \"ìƒˆë¡œìš´ ì•ŒëŒì„ ì˜ˆì•½í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"message\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì•ŒëŒ ì‹œ ì•Œë ¤ì¤„ ë‚´ìš©\"\n",
    "                },\n",
    "                \"year\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"\n",
    "                },\n",
    "                \"month\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì›” (1-12)\"\n",
    "                },\n",
    "                \"day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì¼ (1-31)\"\n",
    "                },\n",
    "                \"hour\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì‹œ (0-23)\"\n",
    "                },\n",
    "                \"minute\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ë¶„ (0-59)\"\n",
    "                },\n",
    "                \"relative_day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"\n",
    "                },\n",
    "                \"week_offset\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"\n",
    "                },\n",
    "                \"day_of_week\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ìš”ì¼ (ì˜ˆ: 'ì›”ìš”ì¼', 'í™”ìš”ì¼', ..., 'ì¼ìš”ì¼')\"\n",
    "                },\n",
    "                \"is_ampm_specified\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"ì‚¬ìš©ìê°€ 'ì˜¤ì „', 'ì˜¤í›„', 'ì•„ì¹¨', 'ë°¤' ë“± ì‹œê°„ì„ ëª…í™•íˆ ì§€ì •í–ˆëŠ”ì§€ ì—¬ë¶€\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"hour\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"delete_alarms\",\n",
    "        \"description\": \"ë“±ë¡ëœ ì•ŒëŒì„ ì‚­ì œí•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"message\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì‚­ì œí•  ì•ŒëŒì˜ ë©”ì‹œì§€ ë‚´ìš©\"\n",
    "                },\n",
    "                \"year\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì—°ë„ (4ìë¦¬ ìˆ«ì)\"\n",
    "                },\n",
    "                \"month\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì›” (1-12)\"\n",
    "                },\n",
    "                \"day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì¼ (1-31)\"\n",
    "                },\n",
    "                \"hour\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì‹œ (0-23)\"\n",
    "                },\n",
    "                \"minute\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ë¶„ (0-59)\"\n",
    "                },\n",
    "                \"relative_day\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´. ì˜¤ëŠ˜=0, ë‚´ì¼=1, ëª¨ë ˆ=2, ê¸€í”¼=3 ë“±ë“±\"\n",
    "                },\n",
    "                \"week_offset\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"ì£¼ ë‹¨ìœ„ ì°¨ì´. ì´ë²ˆ ì£¼=0, ë‹¤ìŒ ì£¼=1\"\n",
    "                },\n",
    "                \"day_of_week\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ìš”ì¼ (ì˜ˆ: 'ì›”ìš”ì¼', 'í™”ìš”ì¼', ..., 'ì¼ìš”ì¼')\"\n",
    "                },\n",
    "                \"is_ampm_specified\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"ì‚¬ìš©ìê°€ 'ì˜¤ì „', 'ì˜¤í›„', 'ì•„ì¹¨', 'ë°¤' ë“± ì‹œê°„ì„ ëª…í™•íˆ ì§€ì •í–ˆëŠ”ì§€ ì—¬ë¶€\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9c9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=2048,  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,  # ë í† í° ëª…ì‹œ\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "def stream_with_langchain(question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        tools=TOOL_DEFINITIONS\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "        skip_prompt=True            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "    )           \n",
    "    \n",
    "    # Threadê°€ ìˆì–´ì•¼ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ë°±ê·¸ë¼ìš´ë“œë¡œ generate()ê°€ ì‹¤í–‰ë˜ê³ \n",
    "    # ì„œë¸Œ ìŠ¤ë ˆë“œì—ì„œ streamerë¡œ í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì½ì„ ìˆ˜ ìˆìŒ\n",
    "    # ë”°ë¼ì„œ ë™ì‹œì— ë³‘ë ¬ë¡œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "\n",
    "    thread = Thread(\n",
    "        target=model.generate, \n",
    "        kwargs=dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    )\n",
    "    thread.start()\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "    \n",
    "    thread.join()\n",
    "    print()     # ì¤„ë°”ê¿ˆ\n",
    "\n",
    "    pattern = r\"<function=(\\w+)>(.+?)</function>\"\n",
    "    match = re.search(pattern, full_response)\n",
    "\n",
    "    if match:\n",
    "        # í•¨ìˆ˜ í˜¸ì¶œ í˜•íƒœ ê°ì§€ë¨\n",
    "        func_name = match.group(1)  # get_weather\n",
    "        args_str = match.group(2) # \"location\": \"Seoul\"\n",
    "\n",
    "        args = json.loads(args_str)\n",
    "\n",
    "        tool_result = TOOLS[func_name](**args)\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps(tool_result, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "        formatted_text_2 = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        inputs2 = tokenizer(\n",
    "            formatted_text_2,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "        )\n",
    "\n",
    "        streamer2 = TextIteratorStreamer(\n",
    "            tokenizer,\n",
    "            skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "            skip_prompt=True            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "        )\n",
    "\n",
    "        thread2 = Thread(\n",
    "            target=model.generate, \n",
    "            kwargs=dict(\n",
    "                input_ids=inputs2[\"input_ids\"].to(model.device),\n",
    "                attention_mask=inputs2[\"attention_mask\"].to(model.device),\n",
    "                generation_config=generation_config,\n",
    "                streamer=streamer2,\n",
    "            )\n",
    "        )\n",
    "        thread2.start()\n",
    "        \n",
    "        final_response = \"\"\n",
    "        for text in streamer2:\n",
    "            print(text, end=\"\", flush=True)\n",
    "            final_response += text\n",
    "        \n",
    "        thread2.join()\n",
    "        print()\n",
    "\n",
    "        return final_response\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c813b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ì•ŒëŒ ì¢€ í™•ì¸í•´ì¤˜\n",
      "A: <function=get_alarms>{}</function>\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA: \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mstream_with_langchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m, in \u001b[0;36mstream_with_langchain\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     69\u001b[0m args_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# \"location\": \"Seoul\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m args \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(args_str)\n\u001b[0;32m---> 73\u001b[0m tool_result \u001b[38;5;241m=\u001b[39m \u001b[43mTOOLS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: full_response\n\u001b[1;32m     78\u001b[0m })\n\u001b[1;32m     79\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(tool_result, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m })\n",
      "File \u001b[0;32m~/work/hori/MyModule.py:189\u001b[0m, in \u001b[0;36mget_alarms\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malarms\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mí˜„ì¬ ì„¤ì •ëœ ì•ŒëŒì´ ì—†ìŠµë‹ˆë‹¤.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ALARM_FILE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 189\u001b[0m     alarms \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mprint\u001b[39m(alarms)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malarms\u001b[39m\u001b[38;5;124m\"\u001b[39m: alarms,\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(alarms),\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì•ŒëŒ ëª©ë¡ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m alarms \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mí˜„ì¬ ì„¤ì •ëœ ì•ŒëŒì´ ì—†ìŠµë‹ˆë‹¤.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "question = \"ì•ŒëŒ ì¢€ í™•ì¸í•´ì¤˜\"\n",
    "print(f\"Q: {question}\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5b4fe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ë‹¬ì„œêµ¬ ë‚ ì”¨ ì–´ë•Œ? ë¹„ì˜¤ë ¤ë‚˜? ìš°ì‚°ì„ ì•ˆ ì±™ê²¼ëŠ”ë° ê±±ì •ì´ë„¤\n",
      "A: <function=get_weather>{\"city\":\"ë‹¬ì„œêµ¬\"}</function>\n",
      "\n",
      "</function>\n",
      "{'city': 'ëŒ€êµ¬ ë‹¬ì„œêµ¬', 'temperature': '4ë„', 'sky_condition': 'íë¦¼', 'precipitation_type': 'ì—†ìŒ'}\n",
      "í˜„ì¬ ëŒ€êµ¬ ë‹¬ì„œêµ¬ëŠ” 4ë„ì´ë©° íë¦° ë‚ ì”¨ì…ë‹ˆë‹¤. ë¹„ ì†Œì‹ì€ ì—†ì§€ë§Œ, ê°‘ìê¸° íë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ìš°ì‚°ì„ ì±™ê¸°ëŠ” ê²Œ ì¢‹ê² ì–´ìš”.\n"
     ]
    }
   ],
   "source": [
    "question = \"ë‹¬ì„œêµ¬ ë‚ ì”¨ ì–´ë•Œ? ë¹„ì˜¤ë ¤ë‚˜? ìš°ì‚°ì„ ì•ˆ ì±™ê²¼ëŠ”ë° ê±±ì •ì´ë„¤\"\n",
    "print(f\"Q: {question}\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd103b3d",
   "metadata": {},
   "source": [
    "### GGUF ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5f8fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.3 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected local model directory: /home/user/work/model/kanana-1.5-2.1b-instruct-2505\n",
      "Found HuggingFace hub cache directory: /home/user/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:38<00:00, 158.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/user/work/kanana_merged_temp`\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ëª¨ë¸ ë° ì•„ë‹µí„° ë¡œë“œ (í•œ ë²ˆì— ë¶ˆëŸ¬ì˜¤ê¸°)\n",
    "adapter_path = \"./outputs/kanana-tools-checkpoint-600\" # í•™ìŠµëœ ì•„ë‹µí„°\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = adapter_path, # ì•„ë‹µí„° ê²½ë¡œë¥¼ ë„£ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ê¹Œì§€ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ í•©ì¹œë‹¤.\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë“œë¡œ ë³€í™˜ (í•„ìˆ˜)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# ì ¯ìŠ¨ìš© GGUFë¡œ ì €ì¥í•˜ê¸°\n",
    "model.save_pretrained_merged(\n",
    "    \"../kanana_merged_temp\",    # ì €ì¥ë  í´ë”ëª…/íŒŒì¼ëª…\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e40bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/work/llama.cpp\n",
      "INFO:hf-to-gguf:Loading model: kanana_merged_temp\n",
      "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:heuristics detected bfloat16 tensor dtype, setting --outtype bf16\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {1792, 128259}\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {1792, 128259}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8064, 1792}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {1792, 8064}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 1792}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {1792, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {1792, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {1792}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 1792\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8064\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 8000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "The tokenizer you are loading from '../kanana_merged_temp' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128001\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_sep_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {# version=v3-llama3.1 #}{%- macro append_new_param_info(param_declaration, comment_info, examples_info, depth) -%}\n",
      "    {%- set offset = \"\" -%}\n",
      "    {%- if depth >= 1 -%}\n",
      "        {%- set offset = \"    \" * depth -%}\n",
      "    {%- endif -%}\n",
      "    {%- if comment_info != \"<|NONE|>\" -%}\n",
      "        {{ \"\\n\" + offset + comment_info }}\n",
      "        {%- if examples_info | length > 0 -%}\n",
      "            {# Append each example info #}\n",
      "            {%- for example in examples_info -%}\n",
      "                {{ \"\\n\" + offset + \"// \" + example|string|replace(\"'\", '\"') }}\n",
      "            {%- endfor -%}\n",
      "        {%- endif -%}\n",
      "    {%- endif -%}\n",
      "    {{ \"\\n\" + offset + param_declaration }}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro convert_data_type(param_type) -%}\n",
      "    {%- if param_type == \"integer\" or param_type == \"float\" -%}\n",
      "        {{ \"number\" }}\n",
      "    {%- else -%}\n",
      "        {{ param_type }}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_param_type(param) -%}\n",
      "    {%- set param_type = \"any\" -%}\n",
      "\n",
      "    {%- if \"type\" in param -%}\n",
      "        {%- set raw_param_type = param[\"type\"] -%}\n",
      "        {%- if raw_param_type is iterable and raw_param_type is not string -%}\n",
      "            {%- set param_type = raw_param_type | join(\" | \") -%}\n",
      "        {%- else -%}\n",
      "            {%- set param_type = raw_param_type -%}\n",
      "        {%- endif -%}\n",
      "        {{ convert_data_type(param_type) }}\n",
      "    {%- elif \"oneOf\" in param -%}\n",
      "        {%- set one_of_types = param[\"oneOf\"]|selectattr(\"type\", \"defined\")|list -%}\n",
      "        {%- set one_of_types = one_of_types|map(attribute=\"type\")|unique|list -%}\n",
      "        {{ convert_data_type(one_of_types | join(\" | \")) }}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_format_param(param) -%}\n",
      "    {%- if \"format\" in param -%}\n",
      "        {{ param[\"format\"] }}\n",
      "    {%- elif \"oneOf\" in param -%}\n",
      "        {%- set formats = [] -%}\n",
      "        {%- for item in param[\"oneOf\"] -%}\n",
      "            {%- if \"format\" in item -%}\n",
      "                {%- if item[\"format\"] == param[\"oneOf\"][-1][\"format\"] -%}\n",
      "                    {{ item[\"format\"] }}\n",
      "                {%- else -%}\n",
      "                    {{ item[\"format\"] + \" or \"}}\n",
      "                {%- endif -%}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ \"<|NONE|>\" }}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_param_info(param) -%}\n",
      "    {%- set param_type = param.get(\"type\", \"any\") -%}\n",
      "    {%- set format_param = get_format_param(param) -%}\n",
      "\n",
      "    {%- if \"description\" in param or \"default\" in param or format_param != \"<|NONE|>\" or param[\"maximum\"] or param[\"minimum\"] or param[\"maxLength\"] or param[\"minLength\"] -%}\n",
      "        {{ \"//\" }}\n",
      "        {%- if \"description\" in param -%}\n",
      "            {%- set desc = param[\"description\"] -%}\n",
      "            {%- if not desc.endswith(\".\") -%}\n",
      "                {%- set desc = desc + \".\" -%}\n",
      "            {%- endif -%}\n",
      "            {{ \" \" + desc }}\n",
      "        {%- endif -%}\n",
      "\n",
      "        {%- if \"default\" in param -%}\n",
      "            {%- set default_value = param[\"default\"] -%}\n",
      "            {%- if param_type == \"string\" -%}\n",
      "                {%- set default_value = '\"' ~ default_value ~ '\"' -%}\n",
      "            {%- endif -%}\n",
      "            {{ \" Default=\" ~ default_value ~ \".\" }}\n",
      "        {%- endif -%}\n",
      "\n",
      "        {%- set format_param = get_format_param(param) -%}\n",
      "        {%- if format_param != \"<|NONE|>\" -%}\n",
      "            {{ \" Format=\" ~ format_param }}\n",
      "        {%- endif -%}\n",
      "\n",
      "        {%- for field, field_name in [(\"maximum\", \"Maximum\"), (\"minimum\", \"Minimum\"), (\"maxLength\", \"Maximum length\"), (\"minLength\", \"Minimum length\")] -%}\n",
      "            {%- if field in param -%}\n",
      "                {{ \" \" + field_name ~ \"=\" ~ param[field] }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ \"<|NONE|>\"}}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_enum_option_str(enum_options) -%}\n",
      "    {%- for v in enum_options -%}\n",
      "        {%- if v is string -%}\n",
      "            {{ '\"' + v + '\"' }}\n",
      "        {%- else -%}\n",
      "            {{ v }}\n",
      "        {%- endif -%}\n",
      "        {%- if enum_options|length > 0 and v != enum_options[-1] -%}\n",
      "            {{ \" | \" }}\n",
      "        {%- endif -%}\n",
      "    {%- endfor -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_array_typescript(param_name, param_dic, depth) -%}\n",
      "    {%- set offset = '' -%}\n",
      "    {%- if depth >= 1 -%}\n",
      "        {%- set offset = \"    \" * depth -%}\n",
      "    {%- endif -%}\n",
      "    {%- set items_info = param_dic.get('items', {}) -%}\n",
      "\n",
      "    {%- if items_info|length == 0 -%}\n",
      "        {%- if param_name -%}\n",
      "            {{ \"\\n\" + offset + param_name + \": []\" }}\n",
      "        {%- else -%}\n",
      "            {{ \"\\n\" + offset + \"[]\" }}\n",
      "        {%- endif -%}\n",
      "    {%- else -%}\n",
      "        {%- set array_type = get_param_type(items_info) -%}\n",
      "        {%- if array_type == 'object' -%}\n",
      "            {%- if param_name -%}\n",
      "                {{ \"\\n\" + offset + param_name + \": {\" }}\n",
      "            {%- else -%}\n",
      "                {{ \"\\n\" + offset + \"{\" }}\n",
      "            {%- endif -%}\n",
      "            {{ get_parameter_typescript(items_info.get('properties', {}), items_info.get('required', []), depth + 1) -}}\n",
      "            {{- \"\\n\" + offset + \"}[]\" }}\n",
      "        {%- elif array_type == 'array' -%}\n",
      "            {%- set item_info = get_array_typescript(None, items_info, depth + 1) -%}\n",
      "            {%- if not param_name -%}\n",
      "                {{ \"\\n\" + item_info + \"[]\" }}\n",
      "            {%- else -%}\n",
      "                {{ \"\\n\" + offset + param_name + \": \" + item_info|trim + \"[]\" }}\n",
      "            {%- endif -%}\n",
      "        {%- else -%}\n",
      "            {%- if 'enum' in items_info -%}\n",
      "                {%- set item_type = get_enum_option_str(items_info['enum']) -%}\n",
      "                {%- if param_name is none -%}\n",
      "                    {{ \"(\" + item_type + \")[]\"}}\n",
      "                {%- else -%}\n",
      "                    {{ \"\\n\" + offset + param_name + \": (\" + item_type + \")[]\" }}\n",
      "                {%- endif -%}\n",
      "            {%- else -%}\n",
      "                {%- if param_name is none -%}\n",
      "                    {{ \"\\n\" + array_type + \"[]\" }}\n",
      "                {%- else -%}\n",
      "                    {{ \"\\n\" + offset + param_name + \": \" + array_type + \"[],\" }}\n",
      "                {%- endif -%}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro get_parameter_typescript(properties, required_params, depth=0) -%}\n",
      "    {%- set res = \"\" -%}\n",
      "    {%- for param_name, param in properties.items() -%}\n",
      "        {%- if param is mapping -%}\n",
      "            {%- set comment_info = get_param_info(param) -%}\n",
      "            {# Param Examples #}\n",
      "            {%- set examples_info = [] -%}\n",
      "            {%- if \"examples\" in param -%}\n",
      "                {%- set examples_info = [\"Example \" + param_name + \":\"] -%}\n",
      "                {%- set examples_info = examples_info + param[\"examples\"] -%}\n",
      "            {%- endif -%}\n",
      "\n",
      "            {# Param Name declaration #}\n",
      "            {%- set param_declaration = param_name -%}\n",
      "            {%- if required_params is iterable and param_name not in required_params -%}\n",
      "                {%- set param_declaration = param_declaration + \"?\" -%}\n",
      "            {%- endif -%}\n",
      "\n",
      "            {%- set param_type = get_param_type(param) -%}\n",
      "\n",
      "            {# Handle indentation based on depth #}\n",
      "            {%- set offset = \"\" -%}\n",
      "            {%- if depth >= 1 -%}\n",
      "                {%- set offset = \"    \" * depth -%}\n",
      "            {%- endif -%}\n",
      "\n",
      "            {%- if param_type == \"object\" -%}\n",
      "                {%- if comment_info != \"<|NONE|>\" -%}\n",
      "                    {{ \"\\n\" + offset + comment_info }}\n",
      "                {%- endif -%}\n",
      "                {%- if examples_info|length > 0 -%}\n",
      "                    {%- for example in examples_info -%}\n",
      "                        {{ \"\\n\" + offset + \"// \" + example|string|replace(\"'\", '\"') }}\n",
      "                    {%- endfor -%}\n",
      "                {%- endif -%}\n",
      "                {%- set param_declaration = param_declaration + \": {\" -%}\n",
      "                {{ \"\\n\" + offset + param_declaration -}}\n",
      "                {{- get_parameter_typescript(param.get(\"properties\", {}), param.get(\"required\", []), depth + 1) -}}\n",
      "                {{- \"\\n\" + offset + \"},\" }}\n",
      "            {%- elif param_type == \"array\" -%}\n",
      "                {%- set item_info = param.get(\"items\", {}) -%}\n",
      "                {%- if \"type\" not in item_info -%}\n",
      "                    {%- set param_declaration = param_declaration + \": [],\" -%}\n",
      "                    {{ append_new_param_info(param_declaration, comment_info, examples_info, depth) }}\n",
      "                {%- else -%}\n",
      "                    {%- if comment_info != \"<|NONE|>\" -%}\n",
      "                        {{ \"\\n\" + offset + comment_info }}\n",
      "                    {%- endif -%}\n",
      "                    {%- if examples_info|length > 0 -%}\n",
      "                        {%- for example in examples_info -%}\n",
      "                            {{ \"\\n\" + offset + \"// \" + example|string|replace(\"'\", '\"') }}\n",
      "                        {%- endfor -%}\n",
      "                    {%- endif -%}\n",
      "                    {%- set array_declaration = get_array_typescript(param_declaration, param, depth) -%}\n",
      "                    {%- if not array_declaration.endswith(\",\") -%}\n",
      "                        {%- set array_declaration = array_declaration + \",\" -%}\n",
      "                    {%- endif -%}\n",
      "                    {{ array_declaration}}\n",
      "                {%- endif -%}\n",
      "            {%- else -%}\n",
      "                {%- if \"enum\" in param -%}\n",
      "                    {%- set param_type = get_enum_option_str(param[\"enum\"]) -%}\n",
      "                {%- endif -%}\n",
      "                {%- if \"nullable\" in param and param[\"nullable\"] -%}\n",
      "                    {%- set param_type = param_type + \" | null\" -%}\n",
      "                {%- endif -%}\n",
      "                {%- set param_declaration = param_declaration + \": \" + param_type + \",\" -%}\n",
      "                {{ append_new_param_info(param_declaration, comment_info, examples_info, depth) }}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- endfor -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro generate_schema_from_functions(functions, namespace='functions') -%}\n",
      "    {{ \"// Supported function definitions that should be called when necessary.\\n\" -}}\n",
      "    {{- \"namespace \" + namespace + \" {\\n\\n\" -}}\n",
      "\n",
      "    {%- for function in functions -%}\n",
      "        {%- if function.get(\"function\") -%}\n",
      "            {%- set function = function.get(\"function\") -%}\n",
      "        {%- endif -%}\n",
      "\n",
      "        {%- set function_name = function.get(\"name\") -%}\n",
      "        {%- if function_name -%}\n",
      "            {%- set description = function.get('description', '') -%}\n",
      "            {%- set parameters = function.get('parameters', {}) -%}\n",
      "            {{- \"// \" + description + \"\\n\" -}}\n",
      "            {{- \"type \" + function_name -}}\n",
      "            {%- if parameters and parameters.get(\"properties\") -%}\n",
      "                {{- \" = (_: {\" -}}\n",
      "                {%- set required_params = parameters.get(\"required\", []) -%}\n",
      "                {{ get_parameter_typescript(parameters.get(\"properties\"), required_params, 0) -}}\n",
      "                {{- \"\\n}) => any;\\n\\n\" }}\n",
      "            {%- else -%}\n",
      "                {{ \" = () => any;\\n\\n\" }}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- endfor -%}\n",
      "    {{ \"} // namespace \" + namespace }}\n",
      "{%- endmacro -%}\n",
      "{%- if not tools is defined -%}\n",
      "    {%- set tools = none -%}\n",
      "{%- endif -%}\n",
      "\n",
      "{%- set has_code_interpreter = tools | selectattr(\"type\", \"equalto\", \"code_interpreter\") | list | length > 0 -%}\n",
      "{%- if has_code_interpreter -%}\n",
      "    {%- set tools = tools | rejectattr(\"type\", \"equalto\", \"code_interpreter\") | list -%}\n",
      "{%- endif -%}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- bos_token + \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if has_code_interpreter %}\n",
      "    {{- \"Environment: ipython\\n\\n\" }}\n",
      "{%- else -%}\n",
      "    {{ \"\\n\"}}\n",
      "{%- endif %}\n",
      "{%- if tools %}\n",
      "    {{- \"\\nYou have access to the following functions:\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {%- if \"type\" in t -%}\n",
      "            {{ \"Use the function '\" + t[\"function\"][\"name\"] + \"' to '\" + t[\"function\"][\"description\"] + \"'\\n\" + t[\"function\"] | tojson() }}\n",
      "        {%- else -%}\n",
      "            {{ \"Use the function '\" + t[\"name\"] + \"' to '\" + t[\"description\"] + \"'\\n\" + t | tojson }}\n",
      "        {%- endif -%}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- '\\nThink very carefully before calling functions.\\nIf a you choose to call a function ONLY reply in the following format:\\n<{start_tag}={function_name}>{parameters}{end_tag}\\nwhere\\n\\nstart_tag => `<function`\\nparameters => a JSON dict with the function argument name as key and function argument value as value.\\nend_tag => `</function>`\\n\\nHere is an example,\\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\\n\\nReminder:\\n- If looking for real time information use relevant functions before falling back to brave_search\\n- Function calls MUST follow the specified format, start with <function= and end with </function>\\n- Required parameters MUST be specified\\n- Only call one function at a time\\n- Put the entire function call reply on one line\\n\\n' -}}\n",
      "{%- endif %}\n",
      "{{- \"<|eot_id|>\" -}}\n",
      "\n",
      "{%- for message in messages -%}\n",
      "    {%- if message['role'] == 'user' or message['role'] == 'system' -%}\n",
      "        {{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}\n",
      "    {%- elif message['role'] == 'tool' -%}\n",
      "        {{ '<|start_header_id|>ipython<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}\n",
      "    {%- else -%}\n",
      "        {%- if (message['content'] and message['content']|length > 0) or ('tool_calls' in message and message['tool_calls'] and message['tool_calls']|length > 0) -%}\n",
      "            {{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'}}\n",
      "        {%- endif -%}\n",
      "        {%- if message['content'] and message['content']|length > 0 -%}\n",
      "            {{ message['content'] }}\n",
      "        {%- endif -%}\n",
      "        {%- if 'tool_calls' in message and message['tool_calls'] and message['tool_calls']|length > 0 -%}\n",
      "            {%- for tool_call in message['tool_calls'] -%}\n",
      "                {%- if tool_call[\"function\"][\"name\"] == \"python\" -%}\n",
      "                    {{ '<|python_tag|>' + tool_call['function']['arguments'] }}\n",
      "                {%- else -%}\n",
      "                    {{ '<function=' + tool_call['function']['name'] + '>' + tool_call['function']['arguments'] + '</function>' }}\n",
      "                {%- endif -%}\n",
      "            {%- endfor -%}\n",
      "            {{ '<|eom_id|>' }}\n",
      "        {%- elif message['content'] and message['content']|length > 0 -%}\n",
      "            {{ '<|eot_id|>' }}\n",
      "        {%- endif -%}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif -%}\n",
      "\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:../kanana_f16.gguf: n_tensors = 291, total_size = 4.6G\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.63G/4.63G [02:44<00:00, 28.1Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to ../kanana_f16.gguf\n",
      "main: build = 7708 (0c3b7a9ef)\n",
      "main: built with GNU 13.3.0 for Linux x86_64\n",
      "main: quantizing '../kanana_f16.gguf' to '../kanana_q4_k_m.gguf' as Q4_K_M\n",
      "llama_model_loader: direct I/O is enabled, disabling mmap\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from ../kanana_f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Kanana_Merged_Temp\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 2.3B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 1792\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8064\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 8000000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128259\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128259]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128259]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_sep_token bool             = false\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {# version=v3-llama3.1 #}{%- macro ap...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type bf16:  226 tensors\n",
      "[   1/ 291]                        output.weight - [ 1792, 128259,     1,     1], type =   bf16, converting to q6_K .. size =   438.39 MiB ->   179.81 MiB\n",
      "[   2/ 291]                   output_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[   3/ 291]                    token_embd.weight - [ 1792, 128259,     1,     1], type =   bf16, converting to q4_K .. size =   438.39 MiB ->   123.30 MiB\n",
      "[   4/ 291]                  blk.0.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[   5/ 291]               blk.0.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[   6/ 291]             blk.0.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[   7/ 291]                  blk.0.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[   8/ 291]                  blk.0.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[   9/ 291]                blk.0.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[  10/ 291]                blk.0.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  11/ 291]                blk.0.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  12/ 291]                  blk.0.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  13/ 291]                  blk.1.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  14/ 291]               blk.1.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  15/ 291]             blk.1.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  16/ 291]                  blk.1.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  17/ 291]                  blk.1.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[  18/ 291]                blk.1.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[  19/ 291]                blk.1.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  20/ 291]                blk.1.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  21/ 291]                  blk.1.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  22/ 291]                  blk.2.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  23/ 291]               blk.2.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  24/ 291]             blk.2.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  25/ 291]                  blk.2.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  26/ 291]                  blk.2.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[  27/ 291]                blk.2.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[  28/ 291]                blk.2.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  29/ 291]                blk.2.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  30/ 291]                  blk.2.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  31/ 291]                  blk.3.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  32/ 291]               blk.3.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  33/ 291]             blk.3.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  34/ 291]                  blk.3.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  35/ 291]                  blk.3.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[  36/ 291]                blk.3.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[  37/ 291]                blk.3.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  38/ 291]                blk.3.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  39/ 291]                  blk.3.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  40/ 291]                  blk.4.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  41/ 291]               blk.4.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  42/ 291]             blk.4.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  43/ 291]                  blk.4.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  44/ 291]                  blk.4.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  45/ 291]                blk.4.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[  46/ 291]                blk.4.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  47/ 291]                blk.4.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  48/ 291]                  blk.4.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  49/ 291]                  blk.5.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  50/ 291]               blk.5.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  51/ 291]             blk.5.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  52/ 291]                  blk.5.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  53/ 291]                  blk.5.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  54/ 291]                blk.5.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[  55/ 291]                blk.5.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  56/ 291]                blk.5.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  57/ 291]                  blk.5.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  58/ 291]                  blk.6.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  59/ 291]               blk.6.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  60/ 291]             blk.6.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  61/ 291]                  blk.6.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  62/ 291]                  blk.6.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[  63/ 291]                blk.6.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[  64/ 291]                blk.6.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  65/ 291]                blk.6.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  66/ 291]                  blk.6.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  67/ 291]                  blk.7.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  68/ 291]               blk.7.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  69/ 291]             blk.7.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  70/ 291]                  blk.7.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  71/ 291]                  blk.7.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  72/ 291]                blk.7.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[  73/ 291]                blk.7.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  74/ 291]                blk.7.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  75/ 291]                  blk.7.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  76/ 291]                  blk.8.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  77/ 291]               blk.8.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  78/ 291]             blk.8.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  79/ 291]                  blk.8.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  80/ 291]                  blk.8.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  81/ 291]                blk.8.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[  82/ 291]                blk.8.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  83/ 291]                blk.8.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  84/ 291]                  blk.8.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  85/ 291]                  blk.9.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  86/ 291]               blk.9.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  87/ 291]             blk.9.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  88/ 291]                  blk.9.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  89/ 291]                  blk.9.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[  90/ 291]                blk.9.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[  91/ 291]                blk.9.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  92/ 291]                blk.9.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  93/ 291]                  blk.9.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[  94/ 291]                 blk.10.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  95/ 291]              blk.10.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[  96/ 291]            blk.10.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  97/ 291]                 blk.10.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[  98/ 291]                 blk.10.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  99/ 291]               blk.10.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 100/ 291]               blk.10.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 101/ 291]               blk.10.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 102/ 291]                 blk.10.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 103/ 291]                 blk.11.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 104/ 291]              blk.11.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 105/ 291]            blk.11.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 106/ 291]                 blk.11.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 107/ 291]                 blk.11.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 108/ 291]               blk.11.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 109/ 291]               blk.11.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 110/ 291]               blk.11.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 111/ 291]                 blk.11.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 112/ 291]                 blk.12.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 113/ 291]              blk.12.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 114/ 291]            blk.12.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 115/ 291]                 blk.12.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 116/ 291]                 blk.12.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 117/ 291]               blk.12.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 118/ 291]               blk.12.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 119/ 291]               blk.12.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 120/ 291]                 blk.12.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 121/ 291]                 blk.13.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 122/ 291]              blk.13.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 123/ 291]            blk.13.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 124/ 291]                 blk.13.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 125/ 291]                 blk.13.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 126/ 291]               blk.13.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 127/ 291]               blk.13.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 128/ 291]               blk.13.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 129/ 291]                 blk.13.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 130/ 291]                 blk.14.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 131/ 291]              blk.14.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 132/ 291]            blk.14.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 133/ 291]                 blk.14.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 134/ 291]                 blk.14.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 135/ 291]               blk.14.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 136/ 291]               blk.14.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 137/ 291]               blk.14.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 138/ 291]                 blk.14.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 139/ 291]                 blk.15.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 140/ 291]              blk.15.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 141/ 291]            blk.15.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 142/ 291]                 blk.15.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 143/ 291]                 blk.15.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 144/ 291]               blk.15.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 145/ 291]               blk.15.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 146/ 291]               blk.15.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 147/ 291]                 blk.15.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 148/ 291]                 blk.16.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 149/ 291]              blk.16.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 150/ 291]            blk.16.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 151/ 291]                 blk.16.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 152/ 291]                 blk.16.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 153/ 291]               blk.16.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 154/ 291]               blk.16.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 155/ 291]               blk.16.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 156/ 291]                 blk.16.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 157/ 291]                 blk.17.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 158/ 291]              blk.17.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 159/ 291]            blk.17.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 160/ 291]                 blk.17.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 161/ 291]                 blk.17.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 162/ 291]               blk.17.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 163/ 291]               blk.17.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 164/ 291]               blk.17.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 165/ 291]                 blk.17.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 166/ 291]                 blk.18.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 167/ 291]              blk.18.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 168/ 291]            blk.18.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 169/ 291]                 blk.18.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 170/ 291]                 blk.18.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 171/ 291]               blk.18.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 172/ 291]               blk.18.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 173/ 291]               blk.18.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 174/ 291]                 blk.18.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 175/ 291]                 blk.19.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 176/ 291]              blk.19.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 177/ 291]            blk.19.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 178/ 291]                 blk.19.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 179/ 291]                 blk.19.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 180/ 291]               blk.19.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 181/ 291]               blk.19.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 182/ 291]               blk.19.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 183/ 291]                 blk.19.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 184/ 291]                 blk.20.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 185/ 291]              blk.20.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 186/ 291]            blk.20.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 187/ 291]                 blk.20.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 188/ 291]                 blk.20.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 189/ 291]               blk.20.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 190/ 291]               blk.20.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 191/ 291]               blk.20.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 192/ 291]                 blk.20.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 193/ 291]                 blk.21.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 194/ 291]              blk.21.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 195/ 291]            blk.21.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 196/ 291]                 blk.21.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 197/ 291]                 blk.21.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 198/ 291]               blk.21.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 199/ 291]               blk.21.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 200/ 291]               blk.21.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 201/ 291]                 blk.21.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 202/ 291]                 blk.22.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 203/ 291]              blk.22.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 204/ 291]            blk.22.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 205/ 291]                 blk.22.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 206/ 291]                 blk.22.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 207/ 291]               blk.22.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 208/ 291]               blk.22.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 209/ 291]               blk.22.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 210/ 291]                 blk.22.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 211/ 291]                 blk.23.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 212/ 291]              blk.23.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 213/ 291]            blk.23.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 214/ 291]                 blk.23.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 215/ 291]                 blk.23.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 216/ 291]               blk.23.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 217/ 291]               blk.23.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 218/ 291]               blk.23.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 219/ 291]                 blk.23.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 220/ 291]                 blk.24.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 221/ 291]              blk.24.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 222/ 291]            blk.24.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 223/ 291]                 blk.24.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 224/ 291]                 blk.24.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 225/ 291]               blk.24.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 226/ 291]               blk.24.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 227/ 291]               blk.24.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 228/ 291]                 blk.24.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 229/ 291]                 blk.25.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 230/ 291]              blk.25.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 231/ 291]            blk.25.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 232/ 291]                 blk.25.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 233/ 291]                 blk.25.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 234/ 291]               blk.25.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 235/ 291]               blk.25.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 236/ 291]               blk.25.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 237/ 291]                 blk.25.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 238/ 291]                 blk.26.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 239/ 291]              blk.26.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 240/ 291]            blk.26.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 241/ 291]                 blk.26.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 242/ 291]                 blk.26.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 243/ 291]               blk.26.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =    27.56 MiB ->     9.47 MiB\n",
      "[ 244/ 291]               blk.26.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 245/ 291]               blk.26.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 246/ 291]                 blk.26.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 247/ 291]                 blk.27.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 248/ 291]              blk.27.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 249/ 291]            blk.27.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 250/ 291]                 blk.27.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 251/ 291]                 blk.27.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 252/ 291]               blk.27.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 253/ 291]               blk.27.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 254/ 291]               blk.27.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 255/ 291]                 blk.27.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 256/ 291]                 blk.28.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 257/ 291]              blk.28.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 258/ 291]            blk.28.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 259/ 291]                 blk.28.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 260/ 291]                 blk.28.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 261/ 291]               blk.28.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 262/ 291]               blk.28.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 263/ 291]               blk.28.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 264/ 291]                 blk.28.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 265/ 291]                 blk.29.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 266/ 291]              blk.29.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 267/ 291]            blk.29.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 268/ 291]                 blk.29.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 269/ 291]                 blk.29.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 270/ 291]               blk.29.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 271/ 291]               blk.29.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 272/ 291]               blk.29.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 273/ 291]                 blk.29.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 274/ 291]                 blk.30.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 275/ 291]              blk.30.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 276/ 291]            blk.30.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 277/ 291]                 blk.30.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 278/ 291]                 blk.30.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 279/ 291]               blk.30.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 280/ 291]               blk.30.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 281/ 291]               blk.30.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 282/ 291]                 blk.30.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 283/ 291]                 blk.31.attn_k.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 284/ 291]              blk.31.attn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 285/ 291]            blk.31.attn_output.weight - [ 3072,  1792,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 286/ 291]                 blk.31.attn_q.weight - [ 1792,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    10.50 MiB ->     2.95 MiB\n",
      "[ 287/ 291]                 blk.31.attn_v.weight - [ 1792,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 288/ 291]               blk.31.ffn_down.weight - [ 8064,  1792,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 8064 x 1792 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    27.56 MiB ->    14.64 MiB\n",
      "[ 289/ 291]               blk.31.ffn_gate.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "[ 290/ 291]               blk.31.ffn_norm.weight - [ 1792,     1,     1,     1], type =    f32, size =    0.007 MiB\n",
      "[ 291/ 291]                 blk.31.ffn_up.weight - [ 1792,  8064,     1,     1], type =   bf16, converting to q4_K .. size =    27.56 MiB ->     7.75 MiB\n",
      "llama_model_quantize_impl: model size  =  4419.21 MiB\n",
      "llama_model_quantize_impl: quant size  =  1444.77 MiB\n",
      "llama_model_quantize_impl: WARNING: 32 of 226 tensor(s) required fallback quantization\n",
      "\n",
      "main: quantize time = 106910.65 ms\n",
      "main:    total time = 106910.65 ms\n"
     ]
    }
   ],
   "source": [
    "# 1. llama.cpp í´ë”ë¡œ ì´ë™ (ì£¼í”¼í„° ì„¸ì…˜ ë‚´ ê²½ë¡œ ë³€ê²½ì€ %cd ì‚¬ìš©)\n",
    "%cd /home/user/work/llama.cpp\n",
    "\n",
    "# 2. f16 ggufë¡œ ë³€í™˜ (!ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…¸ ëª…ë ¹ì–´ ì‹¤í–‰)\n",
    "!python convert_hf_to_gguf.py ../kanana_merged_temp --outfile ../kanana_f16.gguf\n",
    "\n",
    "# 3. q4_k_m ggufë¡œ ë³€í™˜\n",
    "!./build/bin/llama-quantize ../kanana_f16.gguf ../kanana_q4_k_m.gguf q4_k_m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
