{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be328cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch, psutil, os, gc\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from MyModule import get_current_time, get_current_date, convert_to_grid, get_weather, search_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8df172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"../model/kanana-1.5-2.1b-instruct-2505\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93aeadbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2026.1.2 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb461cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a727708eb94477bb1f1ad35c071a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user', 'content': 'ì§€ê¸ˆ ëª‡ ì‹œì§€?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '<function=get_current_time>{}\\n\\n</function>'},\n",
       "  {'role': 'tool', 'content': '{\"hour\":13,\"minute\":20,\"second\":40}'},\n",
       "  {'role': 'assistant', 'content': 'í˜„ì¬ ì‹œê°ì€ 13ì‹œ 20ë¶„ì…ë‹ˆë‹¤.'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = load_dataset(\"json\", data_files=\"./data/train_data_singleturn.jsonl\")\n",
    "dataset2 = load_dataset(\"json\", data_files=\"./data/train_data_function_call_kanana.jsonl\")\n",
    "\n",
    "dataset = concatenate_datasets([\n",
    "    dataset1['train'],\n",
    "    dataset2['train']\n",
    "]).shuffle(seed=42)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3d92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ ë§¤í•‘\n",
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "}\n",
    "\n",
    "# 2ï¸âƒ£ Tools ì •ì˜ (Kananaì— ì „ë‹¬)\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41b4a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f887cb227344938390308e08d28a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5056 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\\n\\nYou have access to the following functions:\\n\\nUse the function \\'get_current_time\\' to \\'í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\\'\\n{\"name\": \"get_current_time\", \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}\\n\\nUse the function \\'get_current_date\\' to \\'í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\\'\\n{\"name\": \"get_current_date\", \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}\\n\\nUse the function \\'get_weather\\' to \\'íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\\'\\n{\"name\": \"get_weather\", \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"ì§€ì—­ ì´ë¦„\"}}, \"required\": [\"city\"]}}\\n\\nUse the function \\'search_address\\' to \\'ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\\'\\n{\"name\": \"search_address\", \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"ê²€ìƒ‰ì–´\"}}, \"required\": [\"location\"]}}\\n\\n\\nThink very carefully before calling functions.\\nIf a you choose to call a function ONLY reply in the following format:\\n<{start_tag}={function_name}>{parameters}{end_tag}\\nwhere\\n\\nstart_tag => `<function`\\nparameters => a JSON dict with the function argument name as key and function argument value as value.\\nend_tag => `</function>`\\n\\nHere is an example,\\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\\n\\nReminder:\\n- If looking for real time information use relevant functions before falling back to brave_search\\n- Function calls MUST follow the specified format, start with <function= and end with </function>\\n- Required parameters MUST be specified\\n- Only call one function at a time\\n- Put the entire function call reply on one line\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nì§€ê¸ˆ ëª‡ ì‹œì§€?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<function=get_current_time>{}\\n\\n</function><|eot_id|><|start_header_id|>ipython<|end_header_id|>\\n\\n{\"hour\":13,\"minute\":20,\"second\":40}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ní˜„ì¬ ì‹œê°ì€ 13ì‹œ 20ë¶„ì…ë‹ˆë‹¤.<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "# assistant_only_loss = Falseì¼ ê²½ìš°\n",
    "def formatting_prompts_func(examples):\n",
    "    \n",
    "    texts = []\n",
    "\n",
    "    for message in examples['messages']:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            tools=TOOL_DEFINITIONS\n",
    "        )\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "    return {'text': texts}\n",
    "\n",
    "formatted_dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "print(formatted_dataset[0])\n",
    "\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.1)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "eval_ds = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d17f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assistant_only_loss = Trueì¼ ê²½ìš°\n",
    "# def formatting_prompts_func(examples):\n",
    "#     if len(examples[\"messages\"]) > 0 and isinstance(examples[\"messages\"][0], list):\n",
    "#         messages_list = examples[\"messages\"] # ì˜ˆ) {\"messages\": [ [ì±„íŒ…1], [ì±„íŒ…2], [ì±„íŒ…3] ]}\n",
    "#     else:\n",
    "#         # ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬ (SFTTrainer ê²€ì¦ìš©) \n",
    "#         messages_list = [examples[\"messages\"]] # ì˜ˆ) {\"messages\": [ì±„íŒ…1]}\n",
    "\n",
    "#     texts = []\n",
    "#     for message in messages_list:\n",
    "#         text = tokenizer.apply_chat_template(\n",
    "#             message,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=False,\n",
    "#             tools=TOOL_DEFINITIONS\n",
    "#         )\n",
    "#         texts.append(text)\n",
    "\n",
    "#     # SFTTrainerì˜ formatting_funcëŠ” ë°˜ë“œì‹œ 'ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸'ë¥¼ ë°˜í™˜í•´ì•¼ í•œë‹¤.\n",
    "#     return texts\n",
    "\n",
    "# split_dataset2 = dataset.train_test_split(test_size=0.1)\n",
    "# train_ds2 = split_dataset2[\"train\"]\n",
    "# eval_ds2 = split_dataset2[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e3a787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3af1279e4aa435d93f8a2f30c74c509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/4550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a8ce93e34642428f096993965ed6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "source": [
    "args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 5,\n",
    "        # max_steps = 400,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate = 5e-5, \n",
    "        bf16 = True,\n",
    "        seed = 1234,\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        output_dir = \"outputs\",\n",
    "\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        # SFTConfigì˜ íŒŒë¼ë¯¸í„°\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        # model_init_kwargs={\"trust_remote_code\": True},\n",
    "        # assistant_only_loss=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    processing_class=tokenizer,\n",
    "    # formatting_func=formatting_prompts_func,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4693ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,550 | Num Epochs = 1 | Total steps = 1,138\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 11,501,568 of 2,098,486,272 (0.55% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1138' max='1138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1138/1138 53:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.275219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>0.249825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.236403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.235100</td>\n",
       "      <td>0.228387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.224229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.220921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.218297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.216113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>0.215195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.214087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.213200</td>\n",
       "      <td>0.213613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1138, training_loss=0.2722814535633751, metrics={'train_runtime': 3214.0768, 'train_samples_per_second': 1.416, 'train_steps_per_second': 0.354, 'total_flos': 2.5308356619446784e+16, 'train_loss': 0.2722814535633751, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851ae62",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3023beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch, psutil, os, gc, re, json\n",
    "from datetime import datetime, timedelta\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import GenerationConfig, TextIteratorStreamer\n",
    "from peft import PeftModel\n",
    "from threading import Thread\n",
    "from memory_monitor import MemoryMonitor\n",
    "import requests\n",
    "from MyModule import get_current_time, get_current_date, get_weather, search_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d0bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before Loading] RAM: 993.74 MB | VRAM: 1185.86 MB\n",
      "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[After Loading] RAM: 1353.05 MB | VRAM: 3302.61 MB\n",
      "[After Generating] RAM: 1715.68 MB | VRAM: 3357.80 MB\n"
     ]
    }
   ],
   "source": [
    "monitor = MemoryMonitor(device_index=0)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "monitor.print_usage(label=\"Before Loading\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"../model/kanana-1.5-2.1b-instruct-2505\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./outputs/checkpoint-1100\")\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "monitor.print_usage(label=\"After Loading\")\n",
    "\n",
    "# 1 token ìƒì„±\n",
    "prompt = \"ì•ˆë…•í•˜ì„¸ìš”.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "_ = model.generate(**inputs, max_new_tokens=1)\n",
    "monitor.print_usage(label=\"After Generating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d677cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ ë§¤í•‘\n",
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "}\n",
    "\n",
    "# 2ï¸âƒ£ Tools ì •ì˜ (Kananaì— ì „ë‹¬)\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9c9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=512,  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,  # ë í† í° ëª…ì‹œ\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "def stream_with_langchain(question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        tools=TOOL_DEFINITIONS\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "        skip_prompt=True            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "    )           \n",
    "    \n",
    "    # Threadê°€ ìˆì–´ì•¼ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ë°±ê·¸ë¼ìš´ë“œë¡œ generate()ê°€ ì‹¤í–‰ë˜ê³ \n",
    "    # ì„œë¸Œ ìŠ¤ë ˆë“œì—ì„œ streamerë¡œ í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì½ì„ ìˆ˜ ìˆìŒ\n",
    "    # ë”°ë¼ì„œ ë™ì‹œì— ë³‘ë ¬ë¡œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "\n",
    "    thread = Thread(\n",
    "        target=model.generate, \n",
    "        kwargs=dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    )\n",
    "    thread.start()\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "    \n",
    "    thread.join()\n",
    "    print()     # ì¤„ë°”ê¿ˆ\n",
    "\n",
    "    pattern = r\"<function=(\\w+)>(.+?)</function>\"\n",
    "    match = re.search(pattern, full_response)\n",
    "\n",
    "    if match:\n",
    "        # í•¨ìˆ˜ í˜¸ì¶œ í˜•íƒœ ê°ì§€ë¨\n",
    "        func_name = match.group(1)  # get_weather\n",
    "        args_str = match.group(2) # \"location\": \"Seoul\"\n",
    "\n",
    "        args = json.loads(args_str)\n",
    "\n",
    "        tool_result = TOOLS[func_name](**args)\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps(tool_result, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "        formatted_text_2 = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        inputs2 = tokenizer(\n",
    "            formatted_text_2,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "        )\n",
    "\n",
    "        streamer2 = TextIteratorStreamer(\n",
    "            tokenizer,\n",
    "            skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "            skip_prompt=True            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "        )\n",
    "\n",
    "        thread2 = Thread(\n",
    "            target=model.generate, \n",
    "            kwargs=dict(\n",
    "                input_ids=inputs2[\"input_ids\"].to(model.device),\n",
    "                attention_mask=inputs2[\"attention_mask\"].to(model.device),\n",
    "                generation_config=generation_config,\n",
    "                streamer=streamer2,\n",
    "            )\n",
    "        )\n",
    "        thread2.start()\n",
    "        \n",
    "        final_response = \"\"\n",
    "        for text in streamer2:\n",
    "            print(text, end=\"\", flush=True)\n",
    "            final_response += text\n",
    "        \n",
    "        thread2.join()\n",
    "        print()\n",
    "\n",
    "        return final_response\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c813b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ëŒ€êµ¬ë°±í™”ì  ì£¼ì†Œ ì•Œë ¤ì¤˜\n",
      "A: <function=search_address>{\"location\":\"ëŒ€êµ¬ë°±í™”ì \"}</function>\n",
      "{'road_address_name': 'ëŒ€êµ¬ ì¤‘êµ¬ ëª…ë•ë¡œ 333', 'place_name': 'ëŒ€êµ¬ë°±í™”ì  ëŒ€ë°±í”„ë¼ìì '}\n",
      "ëŒ€êµ¬ë°±í™”ì ì˜ ì£¼ì†ŒëŠ” ëŒ€êµ¬ ì¤‘êµ¬ ëª…ë•ë¡œ 333ì— ìœ„ì¹˜í•œ 'ëŒ€êµ¬ë°±í™”ì  ëŒ€ë°±í”„ë¼ìì 'ì´ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "question = \"ëŒ€êµ¬ë°±í™”ì  ì£¼ì†Œ ì•Œë ¤ì¤˜\"\n",
    "print(f\"Q: {question}\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5b4fe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ì„œë³€ë™ ì£¼ì†Œ ì•Œë ¤ì¤˜.\n",
      "A: <function=search_address>{\"location\":\"ì„œë³€ë™\"}</function>\n",
      "{'road_address_name': 'ëŒ€êµ¬ ë¶êµ¬ í˜¸êµ­ë¡œ 219', 'place_name': 'ì„œë³€ì˜¨ì²œ'}\n",
      "ì„œë³€ë™ì˜ ì£¼ì†ŒëŠ” ëŒ€êµ¬ ë¶êµ¬ í˜¸êµ­ë¡œ 219ì— ìœ„ì¹˜í•œ 'ì„œë³€ì˜¨ì²œ'ì´ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "question = \"ì„œë³€ë™ ì£¼ì†Œ ì•Œë ¤ì¤˜.\"\n",
    "print(f\"Q: {question}\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd103b3d",
   "metadata": {},
   "source": [
    "### GGUF ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c5f8fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.2 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected local model directory: /home/user/work/model/kanana-1.5-2.1b-instruct-2505\n",
      "Found HuggingFace hub cache directory: /home/user/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:46<00:00, 106.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/user/work/LLM/kanana_merged_temp`\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ëª¨ë¸ ë° ì•„ë‹µí„° ë¡œë“œ (í•œ ë²ˆì— ë¶ˆëŸ¬ì˜¤ê¸°)\n",
    "adapter_path = \"./outputs/checkpoint-1100\" # í•™ìŠµëœ ì•„ë‹µí„°\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = adapter_path, # ì•„ë‹µí„° ê²½ë¡œë¥¼ ë„£ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ê¹Œì§€ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ í•©ì¹œë‹¤.\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# ì ¯ìŠ¨ìš© GGUFë¡œ ì €ì¥í•˜ê¸°\n",
    "model.save_pretrained_merged(\n",
    "    \"kanana_merged_temp\",    # ì €ì¥ë  í´ë”ëª…/íŒŒì¼ëª…\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
