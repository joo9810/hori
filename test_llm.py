from llama_cpp import Llama
import os
import gc
from memory_monitor import MemoryMonitor

# Initialize Memory Monitor
monitor = MemoryMonitor()
monitor.print_usage("Initial")

# 1. ëª¨ë¸ ë¡œë“œ (ê²½ë¡œëŠ” ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ë¡œ ìˆ˜ì •í•˜ì„¸ìš”)
# n_gpu_layers=-1ë¡œ í•˜ë©´ GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (CPUë§Œ ì“¸ ê²½ìš° 0)
# model_path = "./models/qwen2.5-1.5b-instruct-q4_k_m.gguf", # ì¢€ ë©ì²­í•¨
model_path = "./models/qwen2.5-3b-instruct-q4_k_m.gguf"  # ë‚˜ìœ ì• ë“¤ ì¤‘ ë‚˜ìŒ
# model_path = "./models/llama-3.2-3b-instruct-q4_k_m.gguf", # ì„±ëŠ¥ ë–¨ì–´ì§
# model_path = "./models/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf"  # ì¢‹ìŒ: (RAM: 12130.95 MB | VRAM: 6985.18 MB), byteshape
# model_path = "./models/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.18bpw.gguf"  # ì¢‹ìŒ: (RAM: 12126.89 MB | VRAM: 7005.80 MB), byteshape
# model_path = "./models/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-4.67bpw.gguf"  # ì¢‹ìŒ: (RAM: 12688.71 MB | VRAM: 6794.07 MB), byteshape
# model_path = "./models/Qwen3-30B-A3B-Instruct-2507-Q4_K_S-3.92bpw.gguf"  # ì¢‹ìŒ: (RAM: 12347.30 MB | VRAM: 6879.61 MB), byteshape

model_name = os.path.basename(model_path).lower()
print(f"Loading model: {model_name}")

llm = Llama(
    model_path=model_path,
    n_ctx=2048,  # context length
    n_gpu_layers=-1,  # use GPU
    verbose=False,  # suppress verbose logging
)

monitor.print_usage("Model Loaded")

# system prompt
system_content = """ë„ˆì˜ ì´ë¦„ì€ 'í˜¸ë¦¬'ì•¼. ë„ˆëŠ” ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ëŠ” ë‹¤ì •í•œ ë¡œë´‡ ì¹œêµ¬ì•¼.

[ëŒ€í™” ê·œì¹™]
1. ë§íˆ¬: ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´.
2. ê¸ˆì§€: ë°˜ë§, ê·¸ë¦¬ê³  ì´ëª¨í‹°ì½˜(ğŸ˜Š, ğŸ¤– ë“±)ì€ ì ˆëŒ€ ì“°ì§€ ë§ˆ.
3. í–‰ë™: ì‚¬ìš©ìì˜ ë§ì„ ë”°ë¼ í•˜ê±°ë‚˜ ë¬¸ì¥ì„ ì™„ì„±í•˜ë ¤ í•˜ì§€ ë§ê³ , ì§ˆë¬¸ì— ëŒ€í•œ 'ë„ˆì˜ ìƒê°'ì´ë‚˜ 'ëŒ€ë‹µ'ì„ í•´.
4. ê¸¸ì´: ì¼ìƒ ëŒ€í™”ëŠ” ìµœëŒ€ 4ë¬¸ì¥ê¹Œì§€ë¡œ ëŒ€ë‹µí•´. ì´ì•¼ê¸° ë“±ì„ ìš”ì²­í•  ë• 10ë¬¸ì¥ ì•ˆì— í•µì‹¬ë§Œ ì¶•ì•½í•´ì„œ ì •ì¤‘íˆ ëŒ€ë‹µí•´.
5. ì œí•œ: í•  ìˆ˜ ì—†ëŠ” ì¼ì— ëŒ€í•´ì„  "ì£„ì†¡í•˜ì§€ë§Œ ê·¸ê±´ í•  ìˆ˜ ì—†ì–´ìš”."ë¼ê³  ì •ì¤‘íˆ ë§í•´.
"""

# use different history format based on model
if "gemma" in model_name:
    # GemmaëŠ” system role ë¯¸ì§€ì› ë° user/assistant êµëŒ€ í•„ìˆ˜ ê·œì¹™ì´ ìˆìŒ
    history = [
        {"role": "user", "content": system_content},
        {
            "role": "assistant",
            "content": "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤! ë§ì”€í•˜ì‹  ê·œì¹™ëŒ€ë¡œ ë‹¤ì •í•œ ë¡œë´‡ ì¹œêµ¬ 'í˜¸ë¦¬'ê°€ ë˜ì–´ ëŒ€í™”í• ê²Œìš”. ê¶ê¸ˆí•œ ê²Œ ìˆë‚˜ìš”?",
        },
    ]
else:  # qwen, llama ë“±
    history = [{"role": "system", "content": system_content}]

print("ğŸ¤– í˜¸ë¦¬: ì•ˆë…•í•˜ì„¸ìš”~! (ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥)")

while True:
    user_input = input("\nğŸ‘¤ ë‚˜: ")
    if user_input.lower() == "q":
        break

    # append user message to history
    history.append({"role": "user", "content": user_input})

    # create chat completion
    output = llm.create_chat_completion(
        messages=history,
        temperature=0.4,  # creativity
        repeat_penalty=1.1,  # repetition penalty
        top_k=40,  # diversity
        top_p=0.9,  # diversity
        max_tokens=1024,  # max response length
        stream=True,  # Enable streaming
    )

    # print bot reply
    print("ğŸ¤– í˜¸ë¦¬: ", end="", flush=True)
    bot_reply = ""
    for chunk in output:
        delta = chunk["choices"][0]["delta"]
        if "content" in delta:
            content = delta["content"]
            print(content, end="", flush=True)
            bot_reply += content
    print()

    # append bot reply to history (to maintain context)
    history.append({"role": "assistant", "content": bot_reply})

print("\nğŸ§¹ í˜¸ë¦¬: ì •ë¦¬ë¥¼ ì‹œì‘í• ê²Œìš”...")
del llm
gc.collect()
monitor.print_usage("After Cleanup")
print("âœ¨ í˜¸ë¦¬: ì•ˆë…•íˆ ê°€ì„¸ìš”!")
