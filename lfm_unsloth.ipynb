{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be328cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch, psutil, os, gc\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b20fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ëª¨ë¸ ìŠ¤ëƒ…ìƒ· ë‹¤ìš´ë¡œë“œ\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B\",\n",
    "#     local_dir=\"../model/HyperCLOVAX-SEED-Text-Instruct-1.5B\",\n",
    "#     local_dir_use_symlinks=False    # ì‹¤ì œ íŒŒì¼ ë³µì‚¬ë³¸ì„ ë§Œë“¤ê¸° ìœ„í•´ ë°˜ë“œì‹œ False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8df172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Lfm2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"../model/LFM2.5-1.2B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93aeadbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb461cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user',\n",
       "   'content': 'ì¹´ë“œ ì§€ê°‘ì„ ìƒì–´ë²„ë ¤ì„œ ë†í˜‘ì€í–‰ ì„œëŒ€ë¬¸ë³¸ì  ê°€ì•¼ í•´. ì£¼ì†Œ ì¢€ ê°€ë¥´ì³ì¤˜.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '<|tool_call_start|>[search_address(location=\"ë†í˜‘ì€í–‰ ì„œëŒ€ë¬¸ë³¸ì \")]<|tool_call_end|>'},\n",
       "  {'role': 'tool',\n",
       "   'content': '{\"location\":\"ë†í˜‘ì€í–‰ ì„œëŒ€ë¬¸ë³¸ì \",\"address\":\"ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘êµ¬ í†µì¼ë¡œ 120\"}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'ì§€ê°‘ì„ ìƒì–´ë²„ë¦¬ì…”ì„œ ë§ì´ ë‹¹í™©í•˜ì…¨ê² ì–´ìš”. ë†í˜‘ì€í–‰ ì„œëŒ€ë¬¸ë³¸ì  ì£¼ì†ŒëŠ” ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘êµ¬ í†µì¼ë¡œ 120ì…ë‹ˆë‹¤. ë¹¨ë¦¬ ì²˜ë¦¬ë˜ì–´ì„œ ê±±ì •ì„ ëœìœ¼ì…¨ìœ¼ë©´ ì¢‹ê² ë„¤ìš”.'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = load_dataset(\"json\", data_files=\"./data/train_data_singleturn.jsonl\")\n",
    "dataset2 = load_dataset(\"json\", data_files=\"./data/train_data_function_call_lfm.jsonl\")\n",
    "\n",
    "dataset = concatenate_datasets([\n",
    "    dataset1['train'],\n",
    "    dataset2['train']\n",
    "]).shuffle(seed=42)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3d92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ í•¨ìˆ˜ ì •ì˜\n",
    "def get_current_time():\n",
    "    now = datetime.now()\n",
    "    hour = now.hour\n",
    "    minute = now.minute\n",
    "    second = now.second\n",
    "    return {\n",
    "        \"hour\": hour,\n",
    "        \"minute\": minute,\n",
    "        \"second\": second\n",
    "    }\n",
    "\n",
    "def get_current_date():\n",
    "    now = datetime.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "    day = now.day\n",
    "    day_of_week = now.strftime(\"%A\")\n",
    "    return {\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": day,\n",
    "        \"day_of_week\": day_of_week\n",
    "    }\n",
    "\n",
    "def get_weather(city: str):\n",
    "    \"\"\"ë‚ ì”¨ ì •ë³´ (ë”ë¯¸)\"\"\"\n",
    "    # ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ\n",
    "    return {\n",
    "        \"city\": city,\n",
    "        \"weather\": \"ë§‘ìŒ\",\n",
    "        \"temperature\": 25,\n",
    "        \"humidity\": 60\n",
    "    }\n",
    "\n",
    "def search_address(location):\n",
    "    \"\"\"ì£¼ì†Œ ê²€ìƒ‰ (ë”ë¯¸)\"\"\"\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"address\": \"ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ 27ë²ˆê¸¸\"\n",
    "    }\n",
    "\n",
    "# í•¨ìˆ˜ ë§¤í•‘\n",
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "}\n",
    "\n",
    "# 2ï¸âƒ£ Tools ì •ì˜ (Kananaì— ì „ë‹¬)\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cffaeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<|startoftext|><|im_start|>system\\nList of tools: [{\"name\": \"get_current_time\", \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}, {\"name\": \"get_current_date\", \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}, {\"name\": \"get_weather\", \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"ì§€ì—­ ì´ë¦„\"}}, \"required\": [\"city\"]}}, {\"name\": \"search_address\", \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"ê²€ìƒ‰ì–´\"}}, \"required\": [\"location\"]}}]<|im_end|>\\n<|im_start|>user\\nì¹´ë“œ ì§€ê°‘ì„ ìƒì–´ë²„ë ¤ì„œ ë†í˜‘ì€í–‰ ì„œëŒ€ë¬¸ë³¸ì  ê°€ì•¼ í•´. ì£¼ì†Œ ì¢€ ê°€ë¥´ì³ì¤˜.<|im_end|>\\n<|im_start|>assistant\\n<|tool_call_start|>[search_address(location=\"ë†í˜‘ì€í–‰ ì„œëŒ€ë¬¸ë³¸ì \")]<|tool_call_end|><|im_end|>\\n<|im_start|>tool\\n{\"location\":\"ë†í˜‘ì€í–‰ ì„œëŒ€ë¬¸ë³¸ì \",\"address\":\"ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘êµ¬ í†µì¼ë¡œ 120\"}<|im_end|>\\n<|im_start|>assistant\\nì§€ê°‘ì„ ìƒì–´ë²„ë¦¬ì…”ì„œ ë§ì´ ë‹¹í™©í•˜ì…¨ê² ì–´ìš”. ë†í˜‘ì€í–‰ ì„œëŒ€ë¬¸ë³¸ì  ì£¼ì†ŒëŠ” ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘êµ¬ í†µì¼ë¡œ 120ì…ë‹ˆë‹¤. ë¹¨ë¦¬ ì²˜ë¦¬ë˜ì–´ì„œ ê±±ì •ì„ ëœìœ¼ì…¨ìœ¼ë©´ ì¢‹ê² ë„¤ìš”.<|im_end|>\\n'}\n"
     ]
    }
   ],
   "source": [
    "# # assistant_only_loss = Falseì¼ ê²½ìš°\n",
    "# def formatting_prompts_func(examples):\n",
    "#     texts = []\n",
    "\n",
    "#     for message in examples['messages']:\n",
    "#         text = tokenizer.apply_chat_template(\n",
    "#             message,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=False,\n",
    "#             tools=TOOL_DEFINITIONS\n",
    "#         )\n",
    "\n",
    "#         texts.append(text)\n",
    "\n",
    "#     return {'text': texts}\n",
    "\n",
    "# formatted_dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "# print(formatted_dataset[0])\n",
    "\n",
    "# split_dataset = formatted_dataset.train_test_split(test_size=0.1)\n",
    "# train_ds = split_dataset[\"train\"]\n",
    "# eval_ds = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d17f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assistant_only_loss = Trueì¼ ê²½ìš°\n",
    "def formatting_prompts_func(examples):\n",
    "    if len(examples[\"messages\"]) > 0 and isinstance(examples[\"messages\"][0], list):\n",
    "        messages_list = examples[\"messages\"] # ì˜ˆ) {\"messages\": [ [ì±„íŒ…1], [ì±„íŒ…2], [ì±„íŒ…3] ]}\n",
    "    else:\n",
    "        # ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬ (SFTTrainer ê²€ì¦ìš©) \n",
    "        messages_list = [examples[\"messages\"]] # ì˜ˆ) {\"messages\": [ì±„íŒ…1]}\n",
    "\n",
    "    texts = []\n",
    "    for message in messages_list:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            tools=TOOL_DEFINITIONS\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    # SFTTrainerì˜ formatting_funcëŠ” ë°˜ë“œì‹œ 'ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸'ë¥¼ ë°˜í™˜í•´ì•¼ í•œë‹¤.\n",
    "    return texts\n",
    "\n",
    "split_dataset2 = dataset.train_test_split(test_size=0.1)\n",
    "train_ds2 = split_dataset2[\"train\"]\n",
    "eval_ds2 = split_dataset2[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f5c72206a94e5bbc7acd8463e563c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/4050 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbbfa972276475d95a7c8594e5623a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "source": [
    "args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # max_steps = 400,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate = 5e-5, \n",
    "        bf16 = True,\n",
    "        seed = 1234,\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        output_dir = \"outputs\",\n",
    "\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        # SFTConfigì˜ íŒŒë¼ë¯¸í„°\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        # model_init_kwargs={\"trust_remote_code\": True},\n",
    "        assistant_only_loss=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds2,\n",
    "    eval_dataset=eval_ds2,\n",
    "    processing_class=tokenizer,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4693ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,050 | Num Epochs = 1 | Total steps = 507\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 884,736 of 1,171,225,344 (0.08% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [507/507 10:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.330500</td>\n",
       "      <td>1.143022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.744800</td>\n",
       "      <td>0.957420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.884140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0.859726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.863245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Lfm2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=507, training_loss=0.968840162194457, metrics={'train_runtime': 647.2425, 'train_samples_per_second': 6.257, 'train_steps_per_second': 0.783, 'total_flos': 7217205906663936.0, 'train_loss': 0.968840162194457, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851ae62",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3023beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch, psutil, os, gc, re, json\n",
    "from datetime import datetime\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import GenerationConfig, TextIteratorStreamer\n",
    "from peft import PeftModel\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d0bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before] RAM: 993.1 MB | VRAM: 0.0 MB, VRAM peak: 0.0 MB\n",
      "==((====))==  Unsloth 2026.1.2: Fast Lfm2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[after_load] RAM: 1186.1 MB | VRAM: 771.5 MB, VRAM peak: 1108.1 MB\n",
      "[after_gen] RAM: 1717.3 MB | VRAM: 779.6 MB, VRAM peak: 1108.1 MB\n"
     ]
    }
   ],
   "source": [
    "def print_mem(tag):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram = process.memory_full_info().rss / 1024**2\n",
    "    vram = torch.cuda.memory_allocated() / 1024**2\n",
    "    peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    print(f\"[{tag}] RAM: {ram:.1f} MB | VRAM: {vram:.1f} MB, VRAM peak: {peak:.1f} MB\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print_mem(\"before\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"../model/LFM2.5-1.2B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./outputs/checkpoint-500\")\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print_mem(\"after_load\")\n",
    "\n",
    "# 1 token ìƒì„±\n",
    "prompt = \"ì•ˆë…•í•˜ì„¸ìš”.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "_ = model.generate(**inputs, max_new_tokens=1)\n",
    "print_mem(\"after_gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d677cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ í•¨ìˆ˜ ì •ì˜\n",
    "def get_current_time():\n",
    "    now = datetime.now()\n",
    "    hour = now.hour\n",
    "    minute = now.minute\n",
    "    second = now.second\n",
    "    return {\n",
    "        \"hour\": hour,\n",
    "        \"minute\": minute,\n",
    "        \"second\": second\n",
    "    }\n",
    "\n",
    "def get_current_date():\n",
    "    now = datetime.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "    day = now.day\n",
    "    day_of_week = now.strftime(\"%A\")\n",
    "    return {\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": day,\n",
    "        \"day_of_week\": day_of_week\n",
    "    }\n",
    "\n",
    "def get_weather(city: str):\n",
    "    \"\"\"ë‚ ì”¨ ì •ë³´ (ë”ë¯¸)\"\"\"\n",
    "    # ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ\n",
    "    return {\n",
    "        \"city\": city,\n",
    "        \"weather\": \"ë§‘ìŒ\",\n",
    "        \"temperature\": 25,\n",
    "        \"humidity\": 60\n",
    "    }\n",
    "\n",
    "def search_address(location):\n",
    "    \"\"\"ì£¼ì†Œ ê²€ìƒ‰ (ë”ë¯¸)\"\"\"\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"address\": \"ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ 27ë²ˆê¸¸\"\n",
    "    }\n",
    "\n",
    "# í•¨ìˆ˜ ë§¤í•‘\n",
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "}\n",
    "\n",
    "# 2ï¸âƒ£ Tools ì •ì˜ (Kananaì— ì „ë‹¬)\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9c9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.6\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = False\n",
    "generation_config.use_cache = False\n",
    "generation_config.repetition_penalty=1.2\n",
    "\n",
    "def stream_with_langchain(question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"List of tools: {json.dumps(TOOL_DEFINITIONS)}\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "        skip_prompt=True)           # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "    \n",
    "    # Threadê°€ ìˆì–´ì•¼ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ë°±ê·¸ë¼ìš´ë“œë¡œ generate()ê°€ ì‹¤í–‰ë˜ê³ \n",
    "    # ì„œë¸Œ ìŠ¤ë ˆë“œì—ì„œ streamerë¡œ í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì½ì„ ìˆ˜ ìˆìŒ\n",
    "    # ë”°ë¼ì„œ ë™ì‹œì— ë³‘ë ¬ë¡œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "\n",
    "    thread = Thread(\n",
    "        target=model.generate, \n",
    "        kwargs=dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    )\n",
    "    thread.start()\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "    \n",
    "    thread.join()\n",
    "    print()     # ì¤„ë°”ê¿ˆ\n",
    "\n",
    "    pattern = r\"<\\|tool_call_start\\|>\\[(.*?)\\]<\\|tool_call_end\\|>\"\n",
    "    match = re.search(pattern, full_response)\n",
    "\n",
    "    if match:\n",
    "        call = match.group(1) # get_weather(location=\"Seoul\")\n",
    "        func_name, arg_str = call.split(\"(\", 1)\n",
    "        arg_str = arg_str.rstrip(\")\")\n",
    "\n",
    "        args = {}\n",
    "        if arg_str.strip():\n",
    "            for part in arg_str.split(\",\"):\n",
    "                k, v = part.split(\"=\")\n",
    "                args[k.strip()] = v.strip().strip('\"')\n",
    "\n",
    "        tool_result = TOOLS[func_name](**args)\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps(tool_result, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "        formatted_text_2 = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        inputs2 = tokenizer(\n",
    "            formatted_text_2,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "        )\n",
    "\n",
    "        streamer2 = TextIteratorStreamer(\n",
    "            tokenizer,\n",
    "            skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "            skip_prompt=True            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "        )\n",
    "\n",
    "        thread2 = Thread(\n",
    "            target=model.generate, \n",
    "            kwargs=dict(\n",
    "                input_ids=inputs2[\"input_ids\"].to(model.device),\n",
    "                attention_mask=inputs2[\"attention_mask\"].to(model.device),\n",
    "                generation_config=generation_config,\n",
    "                streamer=streamer2,\n",
    "            )\n",
    "        )\n",
    "        thread2.start()\n",
    "\n",
    "        final_response = \"\"\n",
    "        for text in streamer2:\n",
    "            print(text, end=\"\", flush=True)\n",
    "            final_response += text\n",
    "\n",
    "        thread2.join()\n",
    "        print()\n",
    "\n",
    "        return final_response\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2a3ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ë„ˆëŠ” ëˆ„êµ¬ì•¼?\n",
      "A: ì €ëŠ” ì˜¤ëŠ˜ ì•„ì¹¨ì— ì¼ì–´ë‚˜ì„œ ì»¤í”¼ë¥¼ ë§ˆì‹œë©´ì„œ ì£¼ë³€ì„ ì‚´í´ë³´ëŠ” ê²Œ ì œì¼ ì¢‹ì•„ìš”.\n"
     ]
    }
   ],
   "source": [
    "question = \"ë„ˆëŠ” ëˆ„êµ¬ì•¼?\"\n",
    "print(f\"Q: {question}\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd103b3d",
   "metadata": {},
   "source": [
    "### GGUF ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f8fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.2 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsloth_generic_save_pretrained_merged() got an unexpected keyword argument 'quantization_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m adapter_path, \u001b[38;5;66;03m# ì•„ë‹µí„° ê²½ë¡œë¥¼ ë„£ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ê¹Œì§€ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ í•©ì¹œë‹¤.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      9\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ì ¯ìŠ¨ìš© GGUFë¡œ ì €ì¥í•˜ê¸°\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# q4_k_m ë°©ì‹ì€ 8GB ë¨ í™˜ê²½ì—ì„œ ì„±ëŠ¥ ì†ì‹¤ì´ ì ìœ¼ë©´ì„œ ìš©ëŸ‰ì´ ë§¤ìš° ì°©í•¨\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained_merged\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkanana_merged_temp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ì €ì¥ë  í´ë”ëª…/íŒŒì¼ëª…\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_16bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsloth_generic_save_pretrained_merged() got an unexpected keyword argument 'quantization_method'"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ëª¨ë¸ ë° ì•„ë‹µí„° ë¡œë“œ (í•œ ë²ˆì— ë¶ˆëŸ¬ì˜¤ê¸°)\n",
    "adapter_path = \"./outputs/kanana-tools-checkpoint-500\" # í•™ìŠµëœ ì•„ë‹µí„°\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = adapter_path, # ì•„ë‹µí„° ê²½ë¡œë¥¼ ë„£ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ê¹Œì§€ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ í•©ì¹œë‹¤.\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# ì ¯ìŠ¨ìš© GGUFë¡œ ì €ì¥í•˜ê¸°\n",
    "# q4_k_m ë°©ì‹ì€ 8GB ë¨ í™˜ê²½ì—ì„œ ì„±ëŠ¥ ì†ì‹¤ì´ ì ìœ¼ë©´ì„œ ìš©ëŸ‰ì´ ë§¤ìš° ì°©í•¨\n",
    "model.save_pretrained_merged(\n",
    "    \"kanana_merged_temp\",    # ì €ì¥ë  í´ë”ëª…/íŒŒì¼ëª…\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
