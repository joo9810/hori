{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be328cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "import torch, psutil, os, gc\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ëª¨ë¸ ìŠ¤ëƒ…ìƒ· ë‹¤ìš´ë¡œë“œ\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B\",\n",
    "#     local_dir=\"../model/HyperCLOVAX-SEED-Text-Instruct-1.5B\",\n",
    "#     local_dir_use_symlinks=False    # ì‹¤ì œ íŒŒì¼ ë³µì‚¬ë³¸ì„ ë§Œë“¤ê¸° ìœ„í•´ ë°˜ë“œì‹œ False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8df172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Lfm2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"../model/LFM2.5-1.2B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    dtype = None,\n",
    "    max_seq_length = 1024,\n",
    "    load_in_4bit = True,\n",
    "    full_finetuning = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac8c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # LFM for now is just text only\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 16,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb461cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e4f5673dc94a4dbbd2c3bf4aee74da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user', 'content': 'ê²¨ìš¸ì—” ì™œ ì´ë ‡ê²Œ ì ì´ ë§ì´ ì˜¤ëŠ”ì§€ ëª¨ë¥´ê² ì–´.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'ë‚ ì´ ì¶”ì›Œì§€ë©´ ìš°ë¦¬ ëª¸ì´ ì—ë„ˆì§€ë¥¼ ì•„ë¼ë ¤ê³  í™œë™ì„ ì¤„ì´ê¸° ë•Œë¬¸ì´ë˜ìš”. ë‚®ì ì„ ë„ˆë¬´ ì˜¤ë˜ ì£¼ë¬´ì‹œë©´ ë°¤ì— ì ì´ ì•ˆ ì˜¬ ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”.'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = load_dataset(\"json\", data_files=\"./data/train_data_singleturn.jsonl\")\n",
    "dataset2 = load_dataset(\"json\", data_files=\"./data/train_data_function_call_lfm.jsonl\")\n",
    "\n",
    "dataset = concatenate_datasets([\n",
    "    dataset1['train'],\n",
    "    dataset2['train']\n",
    "]).shuffle(seed=42)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3d92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ í•¨ìˆ˜ ì •ì˜\n",
    "def get_current_time():\n",
    "    now = datetime.now()\n",
    "    hour = now.hour\n",
    "    minute = now.minute\n",
    "    second = now.second\n",
    "    return {\n",
    "        \"hour\": hour,\n",
    "        \"minute\": minute,\n",
    "        \"second\": second\n",
    "    }\n",
    "\n",
    "def get_current_date():\n",
    "    now = datetime.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "    day = now.day\n",
    "    day_of_week = now.strftime(\"%A\")\n",
    "    return {\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": day,\n",
    "        \"day_of_week\": day_of_week\n",
    "    }\n",
    "\n",
    "def get_weather(city: str):\n",
    "    \"\"\"ë‚ ì”¨ ì •ë³´ (ë”ë¯¸)\"\"\"\n",
    "    # ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ\n",
    "    return {\n",
    "        \"city\": city,\n",
    "        \"weather\": \"ë§‘ìŒ\",\n",
    "        \"temperature\": 25,\n",
    "        \"humidity\": 60\n",
    "    }\n",
    "\n",
    "def search_address(location):\n",
    "    \"\"\"ì£¼ì†Œ ê²€ìƒ‰ (ë”ë¯¸)\"\"\"\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"address\": \"ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ 27ë²ˆê¸¸\"\n",
    "    }\n",
    "\n",
    "# í•¨ìˆ˜ ë§¤í•‘\n",
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "}\n",
    "\n",
    "# 2ï¸âƒ£ Tools ì •ì˜ (Kananaì— ì „ë‹¬)\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cffaeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c41412e0eb4cc095a3c8c0f999c779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<|startoftext|><|im_start|>system\\nList of tools: [{\"name\": \"get_current_time\", \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}, {\"name\": \"get_current_date\", \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}, {\"name\": \"get_weather\", \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"ì§€ì—­ ì´ë¦„\"}}, \"required\": [\"city\"]}}, {\"name\": \"search_address\", \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"ê²€ìƒ‰ì–´\"}}, \"required\": [\"location\"]}}]<|im_end|>\\n<|im_start|>user\\nê²¨ìš¸ì—” ì™œ ì´ë ‡ê²Œ ì ì´ ë§ì´ ì˜¤ëŠ”ì§€ ëª¨ë¥´ê² ì–´.<|im_end|>\\n<|im_start|>assistant\\në‚ ì´ ì¶”ì›Œì§€ë©´ ìš°ë¦¬ ëª¸ì´ ì—ë„ˆì§€ë¥¼ ì•„ë¼ë ¤ê³  í™œë™ì„ ì¤„ì´ê¸° ë•Œë¬¸ì´ë˜ìš”. ë‚®ì ì„ ë„ˆë¬´ ì˜¤ë˜ ì£¼ë¬´ì‹œë©´ ë°¤ì— ì ì´ ì•ˆ ì˜¬ ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”.<|im_end|>\\n'}\n"
     ]
    }
   ],
   "source": [
    "# assistant_only_loss = Falseì¼ ê²½ìš°\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "\n",
    "    for message in examples['messages']:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            tools=TOOL_DEFINITIONS\n",
    "        )\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "    return {'text': texts}\n",
    "\n",
    "formatted_dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "print(formatted_dataset[0])\n",
    "\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.085)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "eval_ds = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d17f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assistant_only_loss = Trueì¼ ê²½ìš°\n",
    "# def formatting_prompts_func(examples):\n",
    "#     if len(examples[\"messages\"]) > 0 and isinstance(examples[\"messages\"][0], list):\n",
    "#         messages_list = examples[\"messages\"] # ì˜ˆ) {\"messages\": [ [ì±„íŒ…1], [ì±„íŒ…2], [ì±„íŒ…3] ]}\n",
    "#     else:\n",
    "#         # ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬ (SFTTrainer ê²€ì¦ìš©) \n",
    "#         messages_list = [examples[\"messages\"]] # ì˜ˆ) {\"messages\": [ì±„íŒ…1]}\n",
    "\n",
    "#     texts = []\n",
    "#     for message in messages_list:\n",
    "#         text = tokenizer.apply_chat_template(\n",
    "#             message,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=False,\n",
    "#             tools=TOOL_DEFINITIONS\n",
    "#         )\n",
    "#         texts.append(text)\n",
    "\n",
    "#     # SFTTrainerì˜ formatting_funcëŠ” ë°˜ë“œì‹œ 'ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸'ë¥¼ ë°˜í™˜í•´ì•¼ í•œë‹¤.\n",
    "#     return texts\n",
    "\n",
    "# split_dataset2 = dataset.train_test_split(test_size=0.1)\n",
    "# train_ds2 = split_dataset2[\"train\"]\n",
    "# eval_ds2 = split_dataset2[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "038e3bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = eval_ds, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3a787",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You set `assistant_only_loss=True`, but the dataset is not conversational. This option is only supported for conversational datasets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[1;32m      2\u001b[0m         per_device_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      3\u001b[0m         gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m         assistant_only_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatting_prompts_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/trainer.py:364\u001b[0m, in \u001b[0;36m_patch_sft_trainer_auto_packing.<locals>.new_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    360\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Padding-free batching auto-enabled for SFTTrainer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m         )\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m packing_active \u001b[38;5;129;01mand\u001b[39;00m _should_skip_auto_packing_error(exc):\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/trainer.py:270\u001b[0m, in \u001b[0;36m_backwards_compatible_trainer.<locals>.new_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m trainer_kwargs\n\u001b[1;32m    269\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 270\u001b[0m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/hori/unsloth_compiled_cache/UnslothSFTTrainer.py:1408\u001b[0m, in \u001b[0;36mUnslothSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1407\u001b[0m     model\u001b[38;5;241m.\u001b[39mfor_training(use_gradient_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(args, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_checkpointing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m-> 1408\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1423\u001b[0m     model\u001b[38;5;241m.\u001b[39mfor_inference()\n",
      "File \u001b[0;32m~/work/hori/unsloth_compiled_cache/UnslothSFTTrainer.py:849\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    840\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using packing, but the attention implementation is not set to a supported flash attention \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariant. Packing gathers multiple samples into a single sequence, and only the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto one of these supported options.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m     )\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39massistant_only_loss \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_conversational(dataset_sample):\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou set `assistant_only_loss=True`, but the dataset is not conversational. This option is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    851\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported for conversational datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    852\u001b[0m     )\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# Dataset\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# Skip dataset preparation if `skip_prepare_dataset=True` in `dataset_kwargs`, or if it's a VLM, where\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# preprocessing [e.g., image-to-pixel conversion] is too costly and done on the fly instead.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m skip_prepare_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    858\u001b[0m     args\u001b[38;5;241m.\u001b[39mdataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdataset_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_prepare_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_vision_dataset\n\u001b[1;32m    861\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: You set `assistant_only_loss=True`, but the dataset is not conversational. This option is only supported for conversational datasets."
     ]
    }
   ],
   "source": [
    "# args = SFTConfig(\n",
    "#         per_device_train_batch_size = 2,\n",
    "#         gradient_accumulation_steps = 4,\n",
    "#         warmup_steps = 5,\n",
    "#         # max_steps = 400,\n",
    "#         num_train_epochs=1,\n",
    "#         learning_rate = 5e-5, \n",
    "#         bf16 = True,\n",
    "#         seed = 1234,\n",
    "#         weight_decay = 0.01,\n",
    "#         lr_scheduler_type = \"linear\",\n",
    "#         output_dir = \"outputs\",\n",
    "\n",
    "#         logging_steps=100,\n",
    "#         eval_strategy=\"steps\",\n",
    "#         eval_steps=100,\n",
    "#         save_strategy=\"steps\",\n",
    "#         save_steps=100,\n",
    "#         save_total_limit=3,\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"loss\",\n",
    "#         greater_is_better=False,\n",
    "\n",
    "#         # SFTConfigì˜ íŒŒë¼ë¯¸í„°\n",
    "#         packing=False,\n",
    "#         dataset_text_field=\"text\",\n",
    "#         # model_init_kwargs={\"trust_remote_code\": True},\n",
    "#         assistant_only_loss=True,\n",
    "# )\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=train_ds,\n",
    "#     eval_dataset=eval_ds,\n",
    "#     processing_class=tokenizer,\n",
    "#     formatting_func=formatting_prompts_func,\n",
    "#     args=args\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4693ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,796 | Num Epochs = 1 | Total steps = 600\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 9,142,272 of 1,179,482,880 (0.78% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 16:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>0.698410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.673195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.351200</td>\n",
       "      <td>0.668291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.654498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.636569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.632382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Lfm2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.43185358683268227, metrics={'train_runtime': 985.8919, 'train_samples_per_second': 4.865, 'train_steps_per_second': 0.609, 'total_flos': 8746517475648000.0, 'train_loss': 0.43185358683268227, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851ae62",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3023beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, psutil, os, gc, re, json\n",
    "from datetime import datetime\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import GenerationConfig, TextIteratorStreamer\n",
    "from peft import PeftModel\n",
    "from threading import Thread\n",
    "from MyModule import get_current_time, get_current_date, convert_to_grid, get_weather, search_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d0bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before] RAM: 996.2 MB | VRAM: 0.0 MB, VRAM peak: 0.0 MB\n",
      "==((====))==  Unsloth 2026.1.2: Fast Lfm2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[after_load] RAM: 1189.7 MB | VRAM: 803.0 MB, VRAM peak: 1108.1 MB\n",
      "[after_gen] RAM: 1728.2 MB | VRAM: 811.1 MB, VRAM peak: 1108.1 MB\n"
     ]
    }
   ],
   "source": [
    "def print_mem(tag):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram = process.memory_full_info().rss / 1024**2\n",
    "    vram = torch.cuda.memory_allocated() / 1024**2\n",
    "    peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    print(f\"[{tag}] RAM: {ram:.1f} MB | VRAM: {vram:.1f} MB, VRAM peak: {peak:.1f} MB\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print_mem(\"before\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"../model/LFM2.5-1.2B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./outputs/checkpoint-600\")\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print_mem(\"after_load\")\n",
    "\n",
    "# 1 token ìƒì„±\n",
    "prompt = \"ì•ˆë…•í•˜ì„¸ìš”.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "_ = model.generate(**inputs, max_new_tokens=1)\n",
    "print_mem(\"after_gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d677cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ ë§¤í•‘\n",
    "TOOLS = {\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"get_current_date\": get_current_date,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_address\": search_address,\n",
    "}\n",
    "\n",
    "# 2ï¸âƒ£ Tools ì •ì˜ (Kananaì— ì „ë‹¬)\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"get_current_time\",\n",
    "        \"description\": \"í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"íŠ¹ì • ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ì§€ì—­ ì´ë¦„\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_address\",\n",
    "        \"description\": \"ì£¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ê²€ìƒ‰ì–´\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9c9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.6\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = False\n",
    "generation_config.use_cache = False\n",
    "generation_config.repetition_penalty=1.2\n",
    "\n",
    "def stream_with_langchain(question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"List of tools: {json.dumps(TOOL_DEFINITIONS)}\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "        skip_prompt=True)           # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "    \n",
    "    # Threadê°€ ìˆì–´ì•¼ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ë°±ê·¸ë¼ìš´ë“œë¡œ generate()ê°€ ì‹¤í–‰ë˜ê³ \n",
    "    # ì„œë¸Œ ìŠ¤ë ˆë“œì—ì„œ streamerë¡œ í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì½ì„ ìˆ˜ ìˆìŒ\n",
    "    # ë”°ë¼ì„œ ë™ì‹œì— ë³‘ë ¬ë¡œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "\n",
    "    thread = Thread(\n",
    "        target=model.generate, \n",
    "        kwargs=dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    )\n",
    "    thread.start()\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "    \n",
    "    thread.join()\n",
    "    print()     # ì¤„ë°”ê¿ˆ\n",
    "\n",
    "    pattern = r\"<\\|tool_call_start\\|>\\[(.*?)\\]<\\|tool_call_end\\|>\"\n",
    "    match = re.search(pattern, full_response)\n",
    "\n",
    "    if match:\n",
    "        call = match.group(1) # get_weather(location=\"Seoul\")\n",
    "        func_name, arg_str = call.split(\"(\", 1)\n",
    "        arg_str = arg_str.rstrip(\")\")\n",
    "\n",
    "        args = {}\n",
    "        if arg_str.strip():\n",
    "            for part in arg_str.split(\",\"):\n",
    "                k, v = part.split(\"=\")\n",
    "                args[k.strip()] = v.strip().strip('\"')\n",
    "\n",
    "        tool_result = TOOLS[func_name](**args)\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps(tool_result, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "        formatted_text_2 = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        inputs2 = tokenizer(\n",
    "            formatted_text_2,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "        )\n",
    "\n",
    "        streamer2 = TextIteratorStreamer(\n",
    "            tokenizer,\n",
    "            skip_special_tokens=True,   # <|end|> ê°™ì€ íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°\n",
    "            skip_prompt=True            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê±´ë„ˆë›°ê¸° (ì‘ë‹µë§Œ ë³´ê¸°)\n",
    "        )\n",
    "\n",
    "        thread2 = Thread(\n",
    "            target=model.generate, \n",
    "            kwargs=dict(\n",
    "                input_ids=inputs2[\"input_ids\"].to(model.device),\n",
    "                attention_mask=inputs2[\"attention_mask\"].to(model.device),\n",
    "                generation_config=generation_config,\n",
    "                streamer=streamer2,\n",
    "            )\n",
    "        )\n",
    "        thread2.start()\n",
    "\n",
    "        final_response = \"\"\n",
    "        for text in streamer2:\n",
    "            print(text, end=\"\", flush=True)\n",
    "            final_response += text\n",
    "\n",
    "        thread2.join()\n",
    "        print()\n",
    "\n",
    "        return final_response\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0964033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ì§€ê¸ˆ ëª‡ ì‹œë‹ˆ? ë„ˆë¬´ ë°°ê³ í”„ë„¤\n",
      "A: <|tool_call_start|>[get_current_time()]<|tool_call_end|>\n",
      "í˜„ì¬ 14ì‹œ 17ë¶„ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì ì‹¬ì€ ë”°ëœ»í•˜ê²Œ ë¨¹ìœ¼ì‹œëŠ” ê²Œ ì¢‹ê² ì–´ìš”!\n"
     ]
    }
   ],
   "source": [
    "question = \"ì§€ê¸ˆ ëª‡ ì‹œë‹ˆ? ë„ˆë¬´ ë°°ê³ í”„ë„¤\"\n",
    "print(f\"Q: {question}\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd103b3d",
   "metadata": {},
   "source": [
    "### GGUF ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f8fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.2 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsloth_generic_save_pretrained_merged() got an unexpected keyword argument 'quantization_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m adapter_path, \u001b[38;5;66;03m# ì•„ë‹µí„° ê²½ë¡œë¥¼ ë„£ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ê¹Œì§€ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ í•©ì¹œë‹¤.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      9\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ì ¯ìŠ¨ìš© GGUFë¡œ ì €ì¥í•˜ê¸°\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# q4_k_m ë°©ì‹ì€ 8GB ë¨ í™˜ê²½ì—ì„œ ì„±ëŠ¥ ì†ì‹¤ì´ ì ìœ¼ë©´ì„œ ìš©ëŸ‰ì´ ë§¤ìš° ì°©í•¨\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained_merged\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkanana_merged_temp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ì €ì¥ë  í´ë”ëª…/íŒŒì¼ëª…\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_16bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsloth_generic_save_pretrained_merged() got an unexpected keyword argument 'quantization_method'"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ëª¨ë¸ ë° ì•„ë‹µí„° ë¡œë“œ (í•œ ë²ˆì— ë¶ˆëŸ¬ì˜¤ê¸°)\n",
    "adapter_path = \"./outputs/kanana-tools-checkpoint-500\" # í•™ìŠµëœ ì•„ë‹µí„°\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = adapter_path, # ì•„ë‹µí„° ê²½ë¡œë¥¼ ë„£ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ê¹Œì§€ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ í•©ì¹œë‹¤.\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# ì ¯ìŠ¨ìš© GGUFë¡œ ì €ì¥í•˜ê¸°\n",
    "# q4_k_m ë°©ì‹ì€ 8GB ë¨ í™˜ê²½ì—ì„œ ì„±ëŠ¥ ì†ì‹¤ì´ ì ìœ¼ë©´ì„œ ìš©ëŸ‰ì´ ë§¤ìš° ì°©í•¨\n",
    "model.save_pretrained_merged(\n",
    "    \"kanana_merged_temp\",    # ì €ì¥ë  í´ë”ëª…/íŒŒì¼ëª…\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
